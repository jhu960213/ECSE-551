{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Fashion_MNIST_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ2dpmX2hj-8"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkP2ADmhg5RG"
      },
      "source": [
        "# load and mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwJrg6S1pVx_"
      },
      "source": [
        "# Set Data Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdW_LKLepdWl"
      },
      "source": [
        "training_set_pickle_path = \"./Train.pkl\"\n",
        "training_labels_path = \"./TrainLabels.csv\"\n",
        "test_set_pickle_path = \"./Test.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEz_c4RcjBxg"
      },
      "source": [
        "# Navigate To CNN_MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8o7gtUQjHLs"
      },
      "source": [
        "# cd into my director where the files are at\n",
        "%cd '/content/gdrive/My Drive/ECSE_551_Machine_Learning/CNN_MNIST'\n",
        "# list what is in the current directory\n",
        "%ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCHksGa8hpsb"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRUJrKQBh91v"
      },
      "source": [
        "# to see what packages are available in the current server's python\n",
        "# and to see which python we are using\n",
        "%%script bash \n",
        "python --version\n",
        "pip install -U skorch\n",
        "pip install torch==1.6.0 torchvision==0.7.0\n",
        "pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71snkKYhw5_"
      },
      "source": [
        "# importing all relevant libraies\n",
        "import pickle\n",
        "import torchvision.models as models\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random as rand\n",
        "from torch.autograd import Variable\n",
        "# from skorch import NeuralNetClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math as ma\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import torch\n",
        "import pandas as  pd \n",
        "print(f\"Import successful!\")\n",
        "print(f\"Pytorch version: {torch.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJouNpq2PqHM"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UNOqPZLPmRL"
      },
      "source": [
        "# Check device\n",
        "USE_CUDA = 0\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "  USE_CUDA = 1\n",
        "  print(f\"Nvidia Cuda/GPU is available!\")\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQdLJBQgkWao"
      },
      "source": [
        "# Torchvision: Image Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFNahwsckTXX"
      },
      "source": [
        "# Transforms are common image transformations. They can be chained together using Compose.\n",
        "# Here we normalize images img=(img-0.5)/0.5\n",
        "# These values normalize the image tensors to be between -1 and 1\n",
        "# Adding Image augmentation to training set to increase accuracy of CNN \n",
        "mean = 0.5\n",
        "std = 0.5\n",
        "# transforms.RandomRotation(10, resample=PIL.Image.BILINEAR)\n",
        "ImageTransforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([mean], [std]),\n",
        "    transforms.RandomRotation(10, resample=PIL.Image.BILINEAR)\n",
        "])\n",
        "\n",
        "# ImageTransforms = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.1307,), (0.3081,))\n",
        "# ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFUFfV1akweL"
      },
      "source": [
        "# Torchvision: Training Dataset & Dataloader "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQyGks4uCnpE"
      },
      "source": [
        "Target Transform Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb7Xg6lxCnPU"
      },
      "source": [
        "target_tranform = lambda a: a - 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSCIit-LwOaU"
      },
      "source": [
        "Getting Length of Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkWl2LLawM2S"
      },
      "source": [
        "training_set_length = 60000\n",
        "print(f\"Length of training set is: {training_set_length}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFrCLnOrA1J"
      },
      "source": [
        "Creates Train & Validation indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuuR470ZrAah"
      },
      "source": [
        "# Returning training and Validation indices \n",
        "def createTrainValIndices(training_set_length, p_train):\n",
        "\n",
        "  # Creating indices for our original training set \n",
        "  indices = np.linspace(0, training_set_length-1, num=training_set_length, dtype=int)\n",
        "  # print(f\"Original in order indices: \\n{indices}\")\n",
        "  rand.shuffle(indices)\n",
        "  # print(f\"Shuffled indices: \\n{indices}\")\n",
        "\n",
        "  # How to split the training and validation set\n",
        "  train_end_index = ma.floor(training_set_length*p_train)\n",
        "  print(f\"Training index start: {0}, Training index end: {train_end_index}\")\n",
        "  val_end_index = training_set_length\n",
        "  print(f\"Validation index start: {train_end_index}, Validation index end: {val_end_index}\")\n",
        "\n",
        "  # Slicing our original indices to form training and test indices\n",
        "  training_indices = indices[0:train_end_index]\n",
        "  val_indices = indices[train_end_index:val_end_index]\n",
        "\n",
        "  return training_indices, val_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2hS9M7msGeT"
      },
      "source": [
        "Our Custom Training Set Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNZ0BNHkk_JZ"
      },
      "source": [
        "# img_file: the pickle file containing the images\n",
        "# label_file: the .csv file containing the labels\n",
        "# transform: We use it for normalizing images (see above)\n",
        "# idx: This is a binary vector that is useful for creating training and validation set.\n",
        "# It return only samples where idx is True\n",
        "\n",
        "class MyDataSet(Dataset):\n",
        "\n",
        "  # MyDataSet constructor which stores the pickled training images and its labels\n",
        "  def __init__(self, img_file, label_file, transform=None, idx = None, target_transform=None):\n",
        "    self.data = pickle.load( open(img_file, 'rb' ), encoding='bytes')\n",
        "    self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1)[:,1:]\n",
        "    if idx is not None:\n",
        "      self.targets = self.targets[idx]\n",
        "      self.data = self.data[idx]\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  # returns the size of our data set\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  # returns a specific image in the data set by index\n",
        "  def __getitem__(self, index):\n",
        "    img, target = self.data[index], int(self.targets[index])\n",
        "    img = Image.fromarray(img.astype('uint8'), mode='L')\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQNZRDxmVJR"
      },
      "source": [
        "Our Custom Test Set Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5vwT4Cj1LfT"
      },
      "source": [
        "# stores my test set\n",
        "class MyTestSet(Dataset):\n",
        "\n",
        "  # constructor \n",
        "  def __init__(self, img_file, transform=None):\n",
        "    self.data = pickle.load( open(img_file, 'rb' ), encoding='bytes')\n",
        "    self.transform = transform\n",
        "\n",
        "  # returns the size of our data set\n",
        "  def __len__(self):\n",
        "    return self.data.shape[0]\n",
        "\n",
        "  # returns a specific image in the data set by index\n",
        "  def __getitem__(self, index):\n",
        "    img = self.data[index]\n",
        "    img = Image.fromarray(img.astype('uint8'), mode='L')\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuBMX4LsTpuf"
      },
      "source": [
        "# Visualizing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5r7HRLXaem"
      },
      "source": [
        "# Create training dataset\n",
        "batch_size = 100\n",
        "training_indices, val_indices = createTrainValIndices(training_set_length, p_train=0.8)\n",
        "training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                         transform=ImageTransforms, idx=training_indices, target_transform=target_tranform)\n",
        "trainingSetDataLoader = DataLoader(training_set, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_aL_fP2Ts7Z"
      },
      "source": [
        "# Read a batch of data and their labels and display them\n",
        "# Note that since data are transformed, they are between [-1,1]\n",
        "imgs, labels = (next(iter(trainingSetDataLoader)))\n",
        "imgs = np.squeeze(imgs)\n",
        "for i in range(0,batch_size):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.imshow(imgs[i].cpu().numpy(),cmap='gray', vmin=-1, vmax=1) #.transpose()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdn5YixtOw12"
      },
      "source": [
        "# Convolution Neural Network Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAX3nEomCUju"
      },
      "source": [
        "Activation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6JXAPgyCT23"
      },
      "source": [
        "# @titledictionary holding a few common activation functions used in CNNs\n",
        "act_func_dict = {\n",
        "    'Relu': nn.ReLU(True), # defacto standard in deep learning these days\n",
        "    'Sig': nn.Sigmoid(), # may provide vanishing gradient problems in deep NNs\n",
        "    'Tanh': nn.Tanh(), # may provide vanishing gradient problems in deep NNs\n",
        "    'LeakyRelu': nn.LeakyReLU(), # slightly better than ReLU as it solves the problem of \"dead neurons\" in the network\n",
        "    'ELU': nn.ELU()\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LON36fboPlqv"
      },
      "source": [
        "Modified VGG CNN with Dropout Regularization & Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqnCiScIO5ZB"
      },
      "source": [
        "# Our various VGG architecture specifications for each layers input/output sizes\n",
        "# M = MaxPool layer\n",
        "VGG_types = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M',512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256,'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64,'M', 128, 128,'M', 256, 256, 256, 256,'M', 512, 512, 512, 512,'M',512,512,512,512,'M'],\n",
        "}\n",
        "\n",
        "# For our fully connected layers\n",
        "VGG16_Structure_Lin = [4096, 9]\n",
        "\n",
        "# We will base our CNN on the VGG Nets \n",
        "# CNN is composed of two types of layers: \n",
        "# 1) Cov and Pooling - Feature Extraction \n",
        "# 2) Fully Connected Linear - Classification \n",
        "class Fashion_VGG_CNN(nn.Module):\n",
        "\n",
        "  # Constructor\n",
        "  # 9 way classification problem so num classes is 9\n",
        "  # in_channel is 1 becasue our images are gray scale\n",
        "  def __init__(self, in_channels = 1, num_classes = 9, dropout=0.15, vgg_type = \"VGG16\", act_func = act_func_dict['Relu']):\n",
        "    super(Fashion_VGG_CNN, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.conv_layers = self._create_cov_layers(self, VGG_types[vgg_type], act_func)\n",
        "    # This fcs represents the last 3 linear fully connected layers of the VGG16\n",
        "    self.fcs = nn.Sequential(\n",
        "        # so the input to the first linear fully connected layer would be\n",
        "        # H = 64/(2**5), W = 128/(2**5) then H*W*512\n",
        "        nn.Linear(in_features=(512*2*4), out_features=VGG16_Structure_Lin[0]),\n",
        "        act_func,\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=VGG16_Structure_Lin[0], out_features=VGG16_Structure_Lin[0]),\n",
        "        act_func,\n",
        "        nn.Dropout(p=dropout),\n",
        "        nn.Linear(in_features=VGG16_Structure_Lin[0], out_features = num_classes)\n",
        "        )\n",
        "    \n",
        "  # Feed forwarding our images to find outputs \n",
        "  def forward(self, x):\n",
        "\n",
        "    # Sending each image through all of our convolution layers\n",
        "    x = self.conv_layers(x)\n",
        "    # After the last max pool layer we need to flatten image into a linear vector\n",
        "    x = x.view(x.size(0), -1)\n",
        "    # Now send the flattened vector into the last 3 fully connected linear layers\n",
        "    x = self.fcs(x)\n",
        "    m = nn.Softmax(dim=1)\n",
        "    return m(x)\n",
        "\n",
        "  # Creates the convolution layers for us for this CNN\n",
        "  @staticmethod\n",
        "  def _create_cov_layers(self, myArchitecture, act_func):\n",
        "    # image input channels for us its only 1 since it's gray scale\n",
        "    in_channels = self.in_channels\n",
        "    # define a list to hold the layers of the CNN\n",
        "    layers = []\n",
        "    # looping through the architecture to define our layers\n",
        "    for x in myArchitecture:\n",
        "      # if it is convolution layer \n",
        "      if (type(x) == int):\n",
        "        out_channels = x\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
        "                             kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
        "                   nn.BatchNorm2d(x),\n",
        "                   act_func]\n",
        "        in_channels = x\n",
        "      # if it is a max pooling layer\n",
        "      elif (x == 'M'):\n",
        "        layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
        "    # Now return the block containing all these layers stacked one after another sequentially\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  # Weight initializations\n",
        "  def reg_init_weights(self, m):\n",
        "    '''\n",
        "        regular model implementation of weight initialization\n",
        "    '''\n",
        "    if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
        "      nn.init.kaiming_normal_(m.weight)\n",
        "      m.bias.data.fill_(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXK4ABc_VaAC"
      },
      "source": [
        "Simple Fashion CNN with Dropout Regularization & Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4jXrjBmrybP"
      },
      "source": [
        "# define a dictionary holding diff CNN dropout rates for each layer \n",
        "CNN_Dropout_rates = {\n",
        "    \"CNN_1\": [0.15],\n",
        "    \"CNN_2\": [0.15, 0.15],\n",
        "    \"CNN_3\": [0.15, 0.2, 0.3],\n",
        "    \"CNN_4\": [0.15, 0.2, 0.3, 0.4]\n",
        "}\n",
        "\n",
        "# define a dictionary holding diff CNN configs\n",
        "CNN_Configs = {\n",
        "    \"CNN_1\": [32, 'M'],\n",
        "    \"CNN_2\": [32, 'M', 64, 'M'],\n",
        "    \"CNN_3\": [32, 'M', 64, 'M', 128, 'M'],\n",
        "    \"CNN_4\": [32, 'M', 64, 'M', 128, 'M', 256, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class Fashion_Simple_CNN(nn.Module):\n",
        "\n",
        "  # Constructor\n",
        "  # 9 way classification problem so num classes is 9\n",
        "  # in_channel is 1 becasue our images are gray scale\n",
        "  def __init__(self, in_channel = 1, num_classes = 9, cnn_type = \"CNN_3\", kernel_size = (3,3), \n",
        "               act_func = act_func_dict['Relu'], use_dropout_reg = True, use_batch_norm = True):\n",
        "    # super class constructor\n",
        "    super(Fashion_Simple_CNN, self).__init__() \n",
        "    # class variables \n",
        "    self.in_channel = in_channel\n",
        "    self.use_dropout_reg = use_dropout_reg\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "    self.kernel_size = kernel_size\n",
        "    self.conv_layers = self._create_cov_layers(self, CNN_Configs[cnn_type], CNN_Dropout_rates[cnn_type], kernel_size, act_func)\n",
        "\n",
        "    # This fcs represents the last 2 linear fully connected layers of this simple CNN\n",
        "    if (cnn_type == \"CNN_3\"):\n",
        "      self.fcs = nn.Sequential(\n",
        "          # so the input to the first linear fully connected layer would be\n",
        "          # H = 64/(2**3), W = 128/(2**3) then H*W*128\n",
        "          nn.Linear(in_features=(128*8*16), out_features=1024),\n",
        "          nn.Dropout(p=0.25),\n",
        "          nn.Linear(in_features=1024, out_features=num_classes),\n",
        "          )\n",
        "    elif (cnn_type == \"CNN_4\"):\n",
        "      self.fcs = nn.Sequential(\n",
        "          # so the input to the first linear fully connected layer would be\n",
        "          # H = 64/(2**3), W = 128/(2**3) then H*W*256\n",
        "          nn.Linear(in_features=(256*4*8), out_features=1024),\n",
        "          nn.Dropout(p=0.25),\n",
        "          nn.Linear(in_features=1024, out_features=num_classes),\n",
        "          )\n",
        "\n",
        "    elif (cnn_type == \"CNN_2\"):\n",
        "      self.fcs = nn.Sequential(\n",
        "          # so the input to the first linear fully connected layer would be\n",
        "          # H = 64/(2**2), W = 128/(2**2) then H*W*64\n",
        "          nn.Linear(in_features=(64*16*32), out_features=1024),\n",
        "          nn.Dropout(p=0.15),\n",
        "          nn.Linear(in_features=1024, out_features=num_classes)\n",
        "          )\n",
        "\n",
        "    elif (cnn_type == \"CNN_1\"):\n",
        "      self.fcs = nn.Sequential(\n",
        "          # so the input to the first linear fully connected layer would be\n",
        "          # H = 64/(2**1), W = 128/(2**1) then H*W*32\n",
        "          nn.Linear(in_features=(32*32*64), out_features=1024),\n",
        "          nn.Dropout(p=0.25),\n",
        "          nn.Linear(in_features=1024, out_features=num_classes)\n",
        "          )\n",
        "\n",
        "\n",
        "  # feed forward our image data to compute y\n",
        "  def forward(self, x):\n",
        "    # Sending each image through all of our convolution layers\n",
        "    x = self.conv_layers(x)\n",
        "    # After the last max pool layer we need to flatten image into a linear vector\n",
        "    x = x.view(x.size(0), -1)\n",
        "    # Now send the flattened vector into the last 2 fully connected linear layers\n",
        "    # print(x.shape)\n",
        "    x = self.fcs(x)\n",
        "    # We should put an appropriate activation for the output layer.\n",
        "    m = nn.Softmax(dim=1)\n",
        "    return m(x)\n",
        "  \n",
        "  # Creates the convolution layers for us for this CNN\n",
        "  @staticmethod\n",
        "  def _create_cov_layers(self, conv_architecture, conv_dropout_rates, kernel_size, act_func):\n",
        "    \n",
        "    # To keep track of drop out rates used at each conv layer\n",
        "    index = 0\n",
        "    # Define in channel value\n",
        "    in_channel = self.in_channel\n",
        "    # List to hold our layers\n",
        "    layers = []\n",
        "\n",
        "    # Loop to go through our CNN output size specification\n",
        "    for x in conv_architecture:\n",
        "\n",
        "      # If it's a conv layer\n",
        "      if (type(x) == int):\n",
        "        out_channels = x\n",
        "        if (self.use_batch_norm):\n",
        "          layers += [nn.Conv2d(in_channels=in_channel, out_channels=out_channels, \n",
        "                             kernel_size=kernel_size, stride=(1,1), padding=(1,1)),\n",
        "                     nn.BatchNorm2d(x), act_func]\n",
        "        else:\n",
        "          layers += [nn.Conv2d(in_channels=in_channel, out_channels=out_channels, \n",
        "                             kernel_size=kernel_size, stride=(1,1), padding=(1,1)), act_func]\n",
        "        in_channel = x\n",
        "      # if it is a max pooling layer\n",
        "      elif (x == 'M'):\n",
        "        if (self.use_dropout_reg):\n",
        "          layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)), nn.Dropout(p=conv_dropout_rates[index])]\n",
        "          index += 1\n",
        "        else:\n",
        "          layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
        "\n",
        "    # Now return the block containing all these layers stacked one after another sequentially\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  # Weight initializations\n",
        "  def reg_init_weights(self, m):\n",
        "    '''\n",
        "        regular model implementation of weight initialization\n",
        "    '''\n",
        "    if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
        "      nn.init.kaiming_normal_(m.weight)\n",
        "      m.bias.data.fill_(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVOhODtr90aN"
      },
      "source": [
        "Modified CNN From Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW1Xry8A94Gt"
      },
      "source": [
        "class CNN_Tutorial(nn.Module):\n",
        "    # This part defines the layers\n",
        "    def __init__(self, in_channel = 1, num_classes = 9, first_kernel = 3, sec_kernel = 3,  \n",
        "                           act_func = act_func_dict['Relu'], p_dropout=0.15):\n",
        "        super(CNN_Tutorial, self).__init__()\n",
        "        \n",
        "        # Calculate the input size of the first linear layer\n",
        "        first_kernel = first_kernel\n",
        "        cov1_param1 = int((64-first_kernel+1)/2)\n",
        "        cov1_param2 = int((128-first_kernel+1)/2)\n",
        "        sec_kernel = sec_kernel\n",
        "        cov2_param1 = int((cov1_param1-sec_kernel+1)/2)\n",
        "        cov2_param2 = int((cov1_param2-sec_kernel+1)/2)\n",
        "        fc1_input_size = 20 * cov2_param1 * cov2_param2\n",
        "        print(f\"The input size of my first linear layer is: {fc1_input_size}\")\n",
        "\n",
        "        # conv layer 1\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channel, out_channels=10, kernel_size=first_kernel),\n",
        "            nn.BatchNorm2d(10),\n",
        "            act_func,\n",
        "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "        )\n",
        "\n",
        "        # conv layer 2\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=sec_kernel),\n",
        "            nn.BatchNorm2d(20),\n",
        "            act_func,\n",
        "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "        )\n",
        "\n",
        "        # fully connected layers\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(fc1_input_size, 600),\n",
        "            act_func,\n",
        "            nn.Dropout(p=p_dropout),\n",
        "            nn.Linear(600, 120),\n",
        "            act_func,\n",
        "            nn.Linear(120, num_classes)\n",
        "        )\n",
        "\n",
        "    # And this part defines the way they are connected to each other\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fcs(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "    # Weight initializations\n",
        "    def reg_init_weights(self, m):\n",
        "      '''\n",
        "          regular model implementation of weight initialization\n",
        "      '''\n",
        "      if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
        "          nn.init.kaiming_normal_(m.weight)\n",
        "          m.bias.data.fill_(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEXCKXrQ4IAQ"
      },
      "source": [
        "CNN with Weights Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT0XI_goqdD3"
      },
      "source": [
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, channel_sizes, layers, batch_norm, dropout, num_classes):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        modules = []\n",
        "        for block_idx in range(0, len(channel_sizes) -1):\n",
        "            modules.append(nn.Conv2d(channel_sizes[block_idx], channel_sizes[block_idx+1], 3, padding=1, bias=False))\n",
        "            if batch_norm:\n",
        "                modules.append(nn.BatchNorm2d(channel_sizes[block_idx+1]))\n",
        "            modules.append(nn.ReLU(True))\n",
        "            if dropout is not None:\n",
        "                modules.append(nn.Dropout2d(dropout, inplace=False))\n",
        "            if layers > 1:\n",
        "                for layer in range(layers - 1):\n",
        "                    modules.append(nn.Conv2d(channel_sizes[block_idx+1], channel_sizes[block_idx+1], 3, padding=1, bias=False))\n",
        "                    if batch_norm:\n",
        "                        modules.append(nn.BatchNorm2d(channel_sizes[block_idx+1]))\n",
        "                    modules.append(nn.ReLU(True))\n",
        "                    if dropout is not None:\n",
        "                        modules.append(nn.Dropout2d(dropout, inplace=False))\n",
        "            \n",
        "            if block_idx + 1  != len(channel_sizes) - 1:\n",
        "                modules.append(nn.MaxPool2d(2,2))\n",
        "        \n",
        "        self.cnn_core = nn.Sequential(*modules)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.linear = nn.Linear(channel_sizes[-1], num_classes)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_core(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        m = nn.Softmax(dim=1)\n",
        "        return m(x)\n",
        "    \n",
        "    # Weight initializations\n",
        "    def reg_init_weights(self, m):\n",
        "      '''\n",
        "          regular model implementation of weight initialization\n",
        "      '''\n",
        "      if (type(m) == nn.Conv3d or type(m) == nn.Linear):\n",
        "          nn.init.kaiming_normal_(m.weight)\n",
        "          m.bias.data.fill_(0.01)\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsCtDJFZf_hc"
      },
      "source": [
        "# Hyper-parameters & Tunning parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdV69AaQqK3U"
      },
      "source": [
        "# Define all the hyperparameters\n",
        "model_name = \"./VGG16_model_exp.tar\"\n",
        "vgg_type = \"VGG16\"\n",
        "cnn_config = [1,64,128,256]\n",
        "p_train = 0.95\n",
        "step_size = 3 # every number of epochs decrease the learning rate \n",
        "gamma = 0.1 # decrease leanring rate by gamma /10 every number of epochs or so\n",
        "lr = 1e-5\n",
        "patience = 4\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "eps = 1e-5\n",
        "batch_size = 128\n",
        "epoch_start = 1\n",
        "num_epochs = 60\n",
        "momentum = 0.5\n",
        "loss_key = \"CEL\"\n",
        "optimizer_key = \"Adam\"\n",
        "act_func = act_func_dict['LeakyRelu']\n",
        "kernel_size = (3,3)\n",
        "stride = (1,1)\n",
        "first_kernel = 5\n",
        "sec_kernel = 5\n",
        "p_dropout = 0.25\n",
        "num_layers = 1\n",
        "use_batch_norm = True\n",
        "small_train_size = 500\n",
        "small_val_size = 100\n",
        "training_set_length = 60000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQzKV6-_9Zhj"
      },
      "source": [
        "# Loss Functions & Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we_9_dHJ94jt"
      },
      "source": [
        "Dictionary of Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1n-f2Jt9g1l"
      },
      "source": [
        "# function returning our desired loss function\n",
        "def selectLoss(key):\n",
        "  loss_func_dict = {\n",
        "      \"CEL\": nn.CrossEntropyLoss(),\n",
        "      \"KLDL\": nn.KLDivLoss(),\n",
        "      \"NLL\": nn.NLLLoss(),\n",
        "      \"MSE\": nn.MSELoss(),\n",
        "  }\n",
        "  return loss_func_dict.get(key,\"Invalid loss function!\") \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7AeJL58BDzB"
      },
      "source": [
        "Dictionary of Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBhVe4mPBJh8"
      },
      "source": [
        "# function returns our desired optimizer\n",
        "def selectOptimizer(key, model, lr, momentum):\n",
        "  optimizers_dict = {\n",
        "      \"SGD\": optim.SGD(model.parameters(), lr=lr, momentum=momentum),\n",
        "      \"Adam\": optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2), eps=eps),\n",
        "      \"RMS\": optim.RMSprop(model.parameters(), lr=lr, momentum=momentum),\n",
        "      \"AdaG\": optim.Adagrad(model.parameters(), lr=lr),\n",
        "      \"AdaD\": optim.Adadelta(model.parameters(), lr=lr)\n",
        "  }\n",
        "  return optimizers_dict.get(key,\"Invalid optimizer!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa2u381rFFo1"
      },
      "source": [
        "# Create Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "athqOc-ZYiw-"
      },
      "source": [
        "Simple CNN with Weights Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wstUkvJvYiGn"
      },
      "source": [
        "Basic_CNN = BasicCNN(channel_sizes=cnn_config, layers = 1, batch_norm=True, dropout=0.15, num_classes=9)\n",
        "Basic_CNN.apply(Basic_CNN.reg_init_weights)\n",
        "print(Basic_CNN)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYP1sO1CFcmV"
      },
      "source": [
        "VGG Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfPsoSQCFOJs"
      },
      "source": [
        "vgg_CNN = Fashion_VGG_CNN(in_channels=1, num_classes=9, vgg_type=\"VGG11\", act_func = act_func_dict['Relu'])\n",
        "print(vgg_CNN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CzgiDMyFPlU"
      },
      "source": [
        "Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUp2FE-DFPBR"
      },
      "source": [
        "simple_CNN = Fashion_Simple_CNN(in_channel = 1, num_classes = 9, cnn_type = \"CNN_3\", kernel_size = (3,3), \n",
        "                           act_func = act_func_dict['Relu'], use_dropout_reg = True, use_batch_norm = True)\n",
        "print(simple_CNN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAwaVFE3nC0K"
      },
      "source": [
        "# CNN Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxVbAxyh0uUd"
      },
      "source": [
        "# create small validaiton and training indices\n",
        "def createSmallTrainValIndicies(small_train_size, small_val_size, training_set_length):\n",
        "  # Creating indices for our original training set \n",
        "  indices = np.linspace(0, training_set_length-1, num=training_set_length, dtype=int)\n",
        "  # print(f\"Original in order indices: \\n{indices}\")\n",
        "  rand.shuffle(indices)\n",
        "  # print(f\"Shuffled indices: \\n{indices}\")\n",
        "  half_index = ma.floor(len(indices)/2)\n",
        "  small_train_indices = indices[0:small_train_size]\n",
        "  small_val_indices = indices[half_index:half_index + small_val_size]\n",
        "  return small_train_indices, small_val_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJtTTrgVuE9q"
      },
      "source": [
        "Mini-batches for Train & Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AigbMFKVqK-x"
      },
      "source": [
        "# create small train and val indices for slicing our data set\n",
        "small_train_indices, small_val_indices = createSmallTrainValIndicies(small_train_size, small_val_size, training_set_length)\n",
        "print(f\"Train size: {len(small_train_indices)}, Validation size: {len(small_val_indices)}\")\n",
        "\n",
        "# Create small train and small val dataset\n",
        "small_dataset = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                         transform=ImageTransforms, idx=small_train_indices, target_transform=target_tranform)\n",
        "small_dataset.targets = small_dataset.target_transform(small_dataset.targets)\n",
        "small_valset = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                         transform=ImageTransforms, idx=small_val_indices, target_transform=target_tranform)\n",
        "small_valset.targets = small_valset.target_transform(small_valset.targets)\n",
        "\n",
        "# Create small train and validation dataloaaders\n",
        "small_train_loader = DataLoader(small_dataset, batch_size=batch_size, shuffle=True)\n",
        "small_val_loader = DataLoader(small_valset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqMwyPblsaAa"
      },
      "source": [
        "Full Training & Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AgOBSfYp8ce"
      },
      "source": [
        "# Creating training and val indices so our data set class can chop them up appropriately \n",
        "training_indices, val_indices = createTrainValIndices(training_set_length, p_train=p_train)\n",
        "\n",
        "# Create training dataset \n",
        "training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                         transform=ImageTransforms, idx=training_indices, target_transform=target_tranform)\n",
        "training_set.targets = training_set.target_transform(training_set.targets).astype(int)\n",
        "# print(training_set.targets[0:10,:])\n",
        "print(f\"My training set shape is: {training_set.data.shape}\")\n",
        "print(f\"My training set labels shape is: {training_set.targets.shape}\")\n",
        "\n",
        "# Create validation dataset \n",
        "validation_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                           transform=ImageTransforms, idx=val_indices, target_transform=target_tranform)\n",
        "validation_set.targets = validation_set.target_transform(validation_set.targets)\n",
        "validation_set.targets = validation_set.targets.astype(int)\n",
        "# print(validation_set.targets[0:10,:])\n",
        "print(f\"My validation set shape is: {validation_set.data.shape}\")\n",
        "print(f\"My validation set labels shape is: {validation_set.targets.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0k-DmJtCB0"
      },
      "source": [
        "Create Training & Validation Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okfd6Y_YtLI2"
      },
      "source": [
        "# Create training and validation loaders\n",
        "trainingSetDataLoader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "validationSetDataLoader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJzSwZo8wjCi"
      },
      "source": [
        "Create different CNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbOY-dx5qLFV"
      },
      "source": [
        "# Tutorial CNN\n",
        "CNN_Tutorial = CNN_Tutorial(in_channel=1, num_classes=9, \n",
        "                            first_kernel=first_kernel, \n",
        "                            sec_kernel=sec_kernel, act_func=act_func,p_dropout=p_dropout)\n",
        "CNN_Tutorial.apply(CNN_Tutorial.reg_init_weights)\n",
        "# print(CNN_Tutorial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQGA9FP3EWTd"
      },
      "source": [
        "# Basic CNN\n",
        "cnn_basic = BasicCNN(channel_sizes=cnn_config, layers = 1, batch_norm=True, dropout=0, num_classes=9)\n",
        "cnn_basic.apply(cnn_basic.reg_init_weights)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lv5b5mhMnMI"
      },
      "source": [
        "simple_CNN = Fashion_Simple_CNN(in_channel = 1, num_classes = 9, cnn_type = \"CNN_2\", kernel_size = (3,3), \n",
        "                           act_func = act_func_dict['Relu'], use_dropout_reg = True, use_batch_norm = True)\n",
        "simple_CNN.apply(simple_CNN.reg_init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nur2lJQ2Fpv"
      },
      "source": [
        "VGG_CNN = Fashion_VGG_CNN(in_channels=1, num_classes=9, dropout=p_dropout, vgg_type=vgg_type, act_func = act_func)\n",
        "# VGG_CNN.apply(VGG_CNN.reg_init_weights)\n",
        "print(VGG_CNN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y0BQjhShQFq"
      },
      "source": [
        "Set-up Loss Function and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0hiYnNoeNQu"
      },
      "source": [
        "# create a net\n",
        "model = VGG_CNN\n",
        "# print(model)\n",
        "\n",
        "# moving our CNN model into GPU memory\n",
        "if USE_CUDA:\n",
        "  model = model.to(DEVICE)\n",
        "\n",
        "# create loss function\n",
        "loss_function = selectLoss(key=loss_key)\n",
        "\n",
        "# create optimizer and step scheduler\n",
        "optimizer = selectOptimizer(key=optimizer_key, model=model, lr=lr, momentum=momentum)\n",
        "print(optimizer.state_dict)\n",
        "plateau_lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y9byLvkdh34"
      },
      "source": [
        "# define lists to hold our training and validation loss and iterations\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "val_losses = []\n",
        "val_counter = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdo5lsdEz54y"
      },
      "source": [
        "globalTrainCounter = 0\n",
        "globalValidationCounter = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSgyKP7p36b7"
      },
      "source": [
        "# Using Full Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X34cRrZC4CQB"
      },
      "source": [
        "# full training set\n",
        "full_training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                         transform=ImageTransforms, idx=np.arange(60000), target_transform=target_tranform)\n",
        "# doing target transforms\n",
        "full_training_set.targets = full_training_set.target_transform(full_training_set.targets).astype(int)\n",
        "\n",
        "# loader for full training set\n",
        "full_training_loader = DataLoader(full_training_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0agmfN_8nFvw"
      },
      "source": [
        "# GridSearchCV with Skorch For CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaxB0P5PnM2y"
      },
      "source": [
        "# reshaping data \n",
        "ts = full_training_set.data.reshape(-1, 1, 64, 128).astype('float32')\n",
        "ts_labels = full_training_set.targets.reshape((full_training_set.targets.shape[0],)).astype('int64')\n",
        "\n",
        "\n",
        "vs = validation_set.data.reshape(-1, 1, 64, 128).astype('float32')\n",
        "vs_labels = validation_set.targets.reshape((validation_set.targets.shape[0],)).astype('int64')\n",
        "\n",
        "\n",
        "print(vs_labels)\n",
        "print(f\"Training set shape: {ts.shape}, Validation set shape: {vs.shape}\")\n",
        "print(f\"Training labels shape: {ts_labels.shape}, Validation labels shape: {vs_labels.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZtGSwmdnNK9"
      },
      "source": [
        "# fixed random seed and cuda random seed \n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# in_channels = 1, num_classes = 9, dropout=0.15, vgg_type = \"VGG16\", act_func = act_func_dict['Relu']\n",
        "\n",
        "# wrapping my own cnn class in skorch cnn\n",
        "cnn = NeuralNetClassifier(\n",
        "    module = Fashion_VGG_CNN,\n",
        "    module__in_channels = 1,\n",
        "    module__num_classes = 9,\n",
        "    module__dropout = p_dropout,\n",
        "    module__vgg_type = vgg_type,\n",
        "    module__act_func = act_func,\n",
        "    max_epochs=80,\n",
        "    lr=5e-5,\n",
        "    optimizer=torch.optim.Adam,\n",
        "    device=DEVICE,\n",
        "    criterion=torch.nn.CrossEntropyLoss,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR6m2DQ_nNPK"
      },
      "source": [
        "# training with skorch\n",
        "cnn.fit(ts, ts_labels);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCN8nK358jTf"
      },
      "source": [
        "cnn.save_params(\n",
        "    f_params='model.pkl', f_optimizer='opt.pkl', f_history='history.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlvzeNpanNS9"
      },
      "source": [
        "# full data set of my training\n",
        "full_training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
        "                         transform=ImageTransforms, idx=training_indices, target_transform=target_tranform)\n",
        "\n",
        "full_training_set.targets = full_training_set.target_transform(full_training_set.targets)\n",
        "gs_ts = full_training_set.data.reshape(-1, 1, 64, 128).astype('float32')\n",
        "gs_tl = full_training_set.targets.reshape((full_training_set.targets.shape[0],)).astype('int64')\n",
        "print(f\"Training set shape: {gs_ts.shape}, Training Labels shape: {gs_tl.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQqZ9kCnNYL"
      },
      "source": [
        "# doing gridsearch with skortch \n",
        "params = {\n",
        "    'lr': [1e-5, 5e-7],\n",
        "    'max_epochs': [5, 10],\n",
        "    'module__dropout': [0.25, 0.5],\n",
        "}\n",
        "gs = GridSearchCV(cnn, params, refit=False, cv=3, scoring='accuracy', verbose=True)\n",
        "gs.fit(gs_ts, gs_tl)\n",
        "print(gs.best_score_, gs.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS1tc8nVyO95"
      },
      "source": [
        "# Reload Model From Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD0b9w3hEvBP"
      },
      "source": [
        "# if load is true meaning we are starting from where we left off after we stopped training\n",
        "load = False\n",
        "if load:\n",
        "  if USE_CUDA == 0:\n",
        "    # load checkpoint dictionary into CPU\n",
        "    checkpoint = torch.load(str(model_name), map_location=torch.device('cpu'))\n",
        "    epoch_start = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  else:\n",
        "    # load checkpoint dictionary into GPU\n",
        "    checkpoint = torch.load(model_name, map_location=DEVICE)\n",
        "    epoch_start = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  print(f\"Reloaded from checkpoint successfully!\")\n",
        "\n",
        "# printing out the state dict of optimizer\n",
        "print(optimizer.state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsZAIGKLoUR"
      },
      "source": [
        "# CNN Training & Model Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfBFzeA7LyL8"
      },
      "source": [
        "# Training function\n",
        "def train_cnn(model, optimizer, loss_function, train_loader, \n",
        "              train_losses, train_counter, globalTrainCounter):\n",
        "  # Defining the running accuracy of training\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # our model is now in training phase \n",
        "  # ensure's our model will use batch norm layers and dropout layers for training\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    batch_data, batch_labels = data\n",
        "    # Initializing grad to 0 to ensure there is no mixing of graidents among batches\n",
        "    optimizer.zero_grad()\n",
        "    # move a batch of images and it's labels into GPU\n",
        "    batch_data = batch_data.to(DEVICE)\n",
        "    batch_labels = batch_labels.to(DEVICE)\n",
        "    # Forward pass\n",
        "    outputs = model(batch_data)\n",
        "    # Adding to the running sum of accuracy\n",
        "    _,pred = torch.max(outputs.data, 1)\n",
        "    correct += (pred == batch_labels).sum()\n",
        "    total += batch_data.size(0)\n",
        "    # find loss\n",
        "    current_loss = loss_function(outputs, batch_labels)\n",
        "    train_losses.append(current_loss)\n",
        "    train_counter.append(globalTrainCounter+1)\n",
        "    globalTrainCounter += 1\n",
        "    # Propagate error backwards\n",
        "    current_loss.backward()\n",
        "    # Optimize our model parameters and update scheduler\n",
        "    optimizer.step()\n",
        "  # Finding avg accuray and loss here at the end of an epoch\n",
        "  avgTrainAccuracy = (float(correct)/float(total))*100\n",
        "  avgTrainLoss = sum(train_losses)/(len(train_losses))\n",
        "  return avgTrainAccuracy, avgTrainLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLYs1Dd1Og9w"
      },
      "source": [
        "# function used to evaluate our validation accuracy on during training \n",
        "def evaluate_cnn(model, validation_loader, loss_function, \n",
        "                 val_losses, val_counter, globalValCounter):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for val_data, val_labels in validation_loader:\n",
        "      val_data, val_labels = val_data.to(DEVICE), val_labels.to(DEVICE)\n",
        "      val_outputs = model(val_data)\n",
        "      current_loss = loss_function(val_outputs, val_labels)\n",
        "      val_losses.append(current_loss)\n",
        "      val_counter.append(globalValCounter+1)\n",
        "      globalValCounter += 1\n",
        "      _,predicted = torch.max(val_outputs.data, 1)\n",
        "      total += val_labels.size(0)\n",
        "      correct += (predicted == val_labels).sum()\n",
        "  avgValAccuracy = (correct/total)*100\n",
        "  avgValLoss = (sum(val_losses)/len(val_losses))\n",
        "  return avgValAccuracy, avgValLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nuHa2fSyfxM"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVmXTewWev0f"
      },
      "source": [
        "# training converges at around 6 epochs for the tutorial cnn\n",
        "# training for a number of epochs\n",
        "last_epoch_num = 0\n",
        "currentAvgValAccuracy = 0\n",
        "train_acc = []\n",
        "train_loss = []\n",
        "val_acc = []\n",
        "val_loss = []\n",
        "for epoch in range(epoch_start, num_epochs+1):\n",
        "  start = time.time()\n",
        "  # train cnn\n",
        "  avgTrainAccuracy, avgTrainLoss = train_cnn(model=model, optimizer=optimizer, loss_function=loss_function, \n",
        "            train_loader=trainingSetDataLoader, train_losses=train_losses, \n",
        "            train_counter=train_counter, globalTrainCounter=globalTrainCounter)\n",
        "  \n",
        "  # evaluate cnn\n",
        "  avgValAccuracy, avgValLoss = evaluate_cnn(model=model, validation_loader=validationSetDataLoader, \n",
        "               loss_function=loss_function, val_losses=val_losses, \n",
        "               val_counter=val_counter, globalValCounter=globalValidationCounter)\n",
        "\n",
        "  # adding to the list\n",
        "  val_acc.append(avgValAccuracy)\n",
        "  val_loss.append(avgValLoss)\n",
        "  train_acc.append(avgTrainAccuracy)\n",
        "  train_loss.append(avgTrainLoss)\n",
        "\n",
        "  # update plateau scheduler \n",
        "  plateau_lr_scheduler.step(avgTrainLoss)\n",
        "\n",
        "  # print epoch info\n",
        "  print(f\"Train Epoch: {epoch}, Avg Training Loss: {avgTrainLoss}, Avg Training Accuracy: {avgTrainAccuracy}%\")\n",
        "  print(f\"Train Epoch: {epoch}, Avg Validation Loss: {avgValLoss}, Avg Validation Accuracy: {avgValAccuracy}%\\n\")\n",
        "\n",
        "  save model when validation accuracy improves\n",
        "  if (currentAvgValAccuracy < avgValAccuracy):\n",
        "    currentAvgValAccuracy = avgValAccuracy\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, str(model_name), _use_new_zipfile_serialization=False)\n",
        "    print(\"\\nSaving my model!\")\n",
        "\n",
        "  end = time.time()\n",
        "  print(f\"Run time per epoch: {end - start} (s) = {(end - start)/60} (mins)\\n\")\n",
        "  last_epoch_num = epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Szgs_Ot0_4"
      },
      "source": [
        "# Saving results for graphing later\n",
        "results = {\n",
        "    'val_acc': val_acc,\n",
        "    'train_acc': train_acc,\n",
        "    'val_loss': val_loss,\n",
        "    'train_loss': train_loss\n",
        "}\n",
        "# Saving results for graphing later\n",
        "with open(\"./results_vgg16_batch128.pickle\", 'wb') as f:\n",
        "  pickle.dump(results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# # To load the same results\n",
        "# with open(\"./results_vgg13_leakyRelu.pickle\", 'rb') as f:\n",
        "#   results = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Bz4vtN7IlY"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4Gnif7s7Oc3"
      },
      "source": [
        "# saving our models fianlly at the very end after training \n",
        "torch.save({\n",
        "            'epoch': last_epoch_num,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, str(model_name), _use_new_zipfile_serialization=False)\n",
        "print(\"\\nSaving my model!\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXppWGz8PdI6"
      },
      "source": [
        "# Final CNN Training & Prediction on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJYpbz_Rcyc"
      },
      "source": [
        "#TODO: need to finish the predict function\n",
        "# to predict on test set \n",
        "def predictOnTestSet(testSetLoader, model,path =\"./output.csv\"):\n",
        "  predictedLabels = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for test_batch in testSetLoader:\n",
        "      test_batch = test_batch.to(DEVICE)\n",
        "      outputs = model(test_batch)\n",
        "      _,predicted = torch.max(outputs.data, 1)\n",
        "      for x in predicted:\n",
        "        predictedLabels.append(x.item()+5)\n",
        "  dfPredicted = pd.DataFrame(predictedLabels, columns=['class'])\n",
        "  print(f\"Length of predicted list is {len(predictedLabels)}\" )\n",
        "  print(path)\n",
        "  dfPredicted.to_csv(path)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdyRH9fQVBpi"
      },
      "source": [
        "testset = MyTestSet(test_set_pickle_path,transform=ImageTransforms) \n",
        "# print(testset.data.shape)\n",
        "# print(testset.data[0:2])\n",
        "testsetLoader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "imgs = (next(iter(testsetLoader)))\n",
        "# print(imgs.shape)\n",
        "# print(imgs[0:2])\n",
        "predictOnTestSet(testsetLoader, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}