{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ2dpmX2hj-8"
   },
   "source": [
    "# Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkP2ADmhg5RG",
    "outputId": "ac665d80-4061-49be-8c94-cf22136aeb6d"
   },
   "outputs": [],
   "source": [
    "# # load and mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwJrg6S1pVx_"
   },
   "source": [
    "# Set Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pdW_LKLepdWl"
   },
   "outputs": [],
   "source": [
    "training_set_pickle_path = \"./Train.pkl\"\n",
    "training_labels_path = \"./TrainLabels.csv\"\n",
    "test_set_pickle_path = \"./Test.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEz_c4RcjBxg"
   },
   "source": [
    "# Navigate To CNN_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8o7gtUQjHLs",
    "outputId": "09d3753b-e41b-4072-aab2-28bcf8f2ad71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_Fashion_MNIST_Project.ipynb  Test.pkl\r\n",
      "ExampleSubmissionRandom.csv      Train.pkl\r\n",
      "LoadData.ipynb                   TrainLabels.csv\r\n"
     ]
    }
   ],
   "source": [
    "# # cd into my director where the files are at\n",
    "# %cd '/content/gdrive/My Drive/ECSE_551_Machine_Learning/CNN_MNIST'\n",
    "# # list what is in the current directory\n",
    "%ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCHksGa8hpsb"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRUJrKQBh91v",
    "outputId": "0a69a0d0-fa36-4010-c521-89fb1d1ee9f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: skorch in /usr/local/anaconda3/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from skorch) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/anaconda3/lib/python3.8/site-packages (from skorch) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.19.1 in /usr/local/anaconda3/lib/python3.8/site-packages (from skorch) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /usr/local/anaconda3/lib/python3.8/site-packages (from skorch) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.14.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from skorch) (4.47.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.19.1->skorch) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.19.1->skorch) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 1.9.12\n",
      "anaconda-project                   0.8.3\n",
      "applaunchservices                  0.2.1\n",
      "appnope                            0.1.0\n",
      "appscript                          1.1.1\n",
      "argh                               0.26.2\n",
      "asn1crypto                         1.3.0\n",
      "astroid                            2.4.2\n",
      "astropy                            4.0.1.post1\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              19.3.0\n",
      "autopep8                           1.5.3\n",
      "Babel                              2.8.0\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.1\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "beautifulsoup4                     4.9.1\n",
      "bitarray                           1.4.0\n",
      "bkcharts                           0.2\n",
      "bleach                             3.1.5\n",
      "bokeh                              2.1.1\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.3.2\n",
      "brotlipy                           0.7.0\n",
      "certifi                            2020.6.20\n",
      "cffi                               1.14.0\n",
      "chardet                            3.0.4\n",
      "click                              7.1.2\n",
      "cloudpickle                        1.5.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.3\n",
      "conda                              4.9.2\n",
      "conda-build                        3.18.11\n",
      "conda-package-handling             1.7.2\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0.post1\n",
      "cryptography                       2.9.2\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.21\n",
      "cytoolz                            0.10.1\n",
      "dask                               2.20.0\n",
      "decorator                          4.4.2\n",
      "defusedxml                         0.6.0\n",
      "diff-match-patch                   20200713\n",
      "distributed                        2.20.0\n",
      "docutils                           0.16\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.8.3\n",
      "Flask                              1.1.2\n",
      "fsspec                             0.7.4\n",
      "future                             0.18.2\n",
      "gevent                             20.6.2\n",
      "glob2                              0.7\n",
      "gmpy2                              2.0.8\n",
      "greenlet                           0.4.16\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.1\n",
      "idna                               2.10\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "importlib-metadata                 1.7.0\n",
      "intervaltree                       3.0.2\n",
      "ipykernel                          5.3.2\n",
      "ipython                            7.16.1\n",
      "ipython-genutils                   0.2.0\n",
      "ipywidgets                         7.5.1\n",
      "isort                              4.3.21\n",
      "itsdangerous                       1.1.0\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.17.1\n",
      "Jinja2                             2.11.2\n",
      "joblib                             0.16.0\n",
      "json5                              0.9.5\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     6.1.6\n",
      "jupyter-console                    6.1.0\n",
      "jupyter-core                       4.6.3\n",
      "jupyterlab                         2.1.5\n",
      "jupyterlab-server                  1.2.0\n",
      "keyring                            21.2.1\n",
      "kiwisolver                         1.2.0\n",
      "lazy-object-proxy                  1.4.3\n",
      "libarchive-c                       2.9\n",
      "llvmlite                           0.33.0+1.g022ab0f\n",
      "locket                             0.2.0\n",
      "lxml                               4.5.2\n",
      "MarkupSafe                         1.1.1\n",
      "matplotlib                         3.2.2\n",
      "mccabe                             0.6.1\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.1.0\n",
      "mkl-random                         1.1.1\n",
      "mkl-service                        2.3.0\n",
      "mock                               4.0.2\n",
      "more-itertools                     8.4.0\n",
      "mpmath                             1.1.0\n",
      "msgpack                            1.0.0\n",
      "multipledispatch                   0.6.0\n",
      "navigator-updater                  0.2.1\n",
      "nbconvert                          5.6.1\n",
      "nbformat                           5.0.7\n",
      "networkx                           2.4\n",
      "nltk                               3.5\n",
      "nose                               1.3.7\n",
      "notebook                           6.0.3\n",
      "numba                              0.50.1\n",
      "numexpr                            2.7.1\n",
      "numpy                              1.18.5\n",
      "numpydoc                           1.1.0\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.4\n",
      "packaging                          20.4\n",
      "pandas                             1.0.5\n",
      "pandocfilters                      1.4.2\n",
      "parso                              0.7.0\n",
      "partd                              1.1.0\n",
      "path                               13.1.0\n",
      "pathlib2                           2.3.5\n",
      "pathtools                          0.1.2\n",
      "patsy                              0.5.1\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             7.2.0\n",
      "pip                                20.1.1\n",
      "pkginfo                            1.5.0.1\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "prometheus-client                  0.8.0\n",
      "prompt-toolkit                     3.0.5\n",
      "psutil                             5.7.0\n",
      "ptyprocess                         0.6.0\n",
      "py                                 1.9.0\n",
      "pybind11                           2.5.0\n",
      "pycodestyle                        2.6.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.20\n",
      "pycurl                             7.43.0.5\n",
      "pydocstyle                         5.0.2\n",
      "pyflakes                           2.2.0\n",
      "Pygments                           2.6.1\n",
      "pylint                             2.5.3\n",
      "pyodbc                             4.0.0-unsupported\n",
      "pyOpenSSL                          19.1.0\n",
      "pyparsing                          2.4.7\n",
      "pyrsistent                         0.16.0\n",
      "PySocks                            1.7.1\n",
      "pytest                             5.4.3\n",
      "python-dateutil                    2.8.1\n",
      "python-jsonrpc-server              0.3.4\n",
      "python-language-server             0.34.1\n",
      "pytz                               2020.1\n",
      "PyWavelets                         1.1.1\n",
      "PyYAML                             5.3.1\n",
      "pyzmq                              19.0.1\n",
      "QDarkStyle                         2.8.1\n",
      "QtAwesome                          0.7.2\n",
      "qtconsole                          4.7.5\n",
      "QtPy                               1.9.0\n",
      "regex                              2020.6.8\n",
      "requests                           2.24.0\n",
      "rope                               0.17.0\n",
      "Rtree                              0.9.4\n",
      "ruamel-yaml                        0.15.87\n",
      "scikit-image                       0.16.2\n",
      "scikit-learn                       0.23.1\n",
      "scipy                              1.5.0\n",
      "seaborn                            0.10.1\n",
      "Send2Trash                         1.5.0\n",
      "setuptools                         49.2.0.post20200714\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     3.4.0.3\n",
      "six                                1.15.0\n",
      "skorch                             0.9.0\n",
      "snowballstemmer                    2.0.0\n",
      "sortedcollections                  1.2.1\n",
      "sortedcontainers                   2.2.2\n",
      "soupsieve                          2.0.1\n",
      "Sphinx                             3.1.2\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             1.0.3\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.4\n",
      "sphinxcontrib-websupport           1.2.3\n",
      "spyder                             4.1.4\n",
      "spyder-kernels                     1.9.2\n",
      "SQLAlchemy                         1.3.18\n",
      "statsmodels                        0.11.1\n",
      "sympy                              1.6.1\n",
      "tables                             3.6.1\n",
      "tabulate                           0.8.7\n",
      "tblib                              1.6.0\n",
      "terminado                          0.8.3\n",
      "testpath                           0.4.4\n",
      "threadpoolctl                      2.1.0\n",
      "toml                               0.10.1\n",
      "toolz                              0.10.0\n",
      "torch                              1.6.0\n",
      "torchvision                        0.7.0\n",
      "tornado                            6.0.4\n",
      "tqdm                               4.47.0\n",
      "traitlets                          4.3.3\n",
      "typing-extensions                  3.7.4.2\n",
      "ujson                              1.35\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.25.9\n",
      "watchdog                           0.10.3\n",
      "wcwidth                            0.2.5\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           1.0.1\n",
      "wheel                              0.34.2\n",
      "widgetsnbextension                 3.5.1\n",
      "wrapt                              1.11.2\n",
      "wurlitzer                          2.0.1\n",
      "xlrd                               1.2.0\n",
      "XlsxWriter                         1.2.9\n",
      "xlwings                            0.19.5\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yapf                               0.30.0\n",
      "zict                               2.0.0\n",
      "zipp                               3.1.0\n",
      "zope.event                         4.4\n",
      "zope.interface                     4.7.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# # to see what packages are available in the current server's python\n",
    "# # and to see which python we are using\n",
    "# %%script bash \n",
    "# python --version\n",
    "# pip install ray\n",
    "%pip install -U skorch\n",
    "# # pip install torch==1.6.0 torchvision==0.7.0\n",
    "# pip list\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E71snkKYhw5_",
    "outputId": "cca9db8f-2a6e-4c60-da40-62ff6ca5495c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n",
      "Pytorch version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# importing all relevant libraies\n",
    "import pickle\n",
    "import torchvision.models as models\n",
    "import time\n",
    "# import cv2 as cv\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as rand\n",
    "from torch.autograd import Variable\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math as ma\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torch\n",
    "# from ray import tune\n",
    "# from ray.tune import CLIReporter\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "print(f\"Import successful!\")\n",
    "print(f\"Pytorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJouNpq2PqHM"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7UNOqPZLPmRL"
   },
   "outputs": [],
   "source": [
    "# Check device\n",
    "USE_CUDA = 0\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "  USE_CUDA = 1\n",
    "  print(f\"Nvidia Cuda/GPU is available!\")\n",
    "\n",
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "#   print('and then re-execute this cell.')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQdLJBQgkWao"
   },
   "source": [
    "# Torchvision: Image Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oFNahwsckTXX"
   },
   "outputs": [],
   "source": [
    "# Transforms are common image transformations. They can be chained together using Compose.\n",
    "# Here we normalize images img=(img-0.5)/0.5\n",
    "# These values normalize the image tensors to be between -1 and 1\n",
    "# Adding Image augmentation to training set to increase accuracy of CNN \n",
    "mean = 0.5\n",
    "std = 0.5\n",
    "# transforms.RandomRotation(10, resample=PIL.Image.BILINEAR)\n",
    "ImageTransforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([mean], [std])\n",
    "])\n",
    "\n",
    "# ImageTransforms = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFUFfV1akweL"
   },
   "source": [
    "# Torchvision: Training Dataset & Dataloader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQyGks4uCnpE"
   },
   "source": [
    "Target Transform Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Yb7Xg6lxCnPU"
   },
   "outputs": [],
   "source": [
    "target_tranform = lambda a: a - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSCIit-LwOaU"
   },
   "source": [
    "Getting Length of Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkWl2LLawM2S",
    "outputId": "2b0cae4c-abc0-439e-9481-6cadee8c6fb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set is: 60000\n"
     ]
    }
   ],
   "source": [
    "training_set_length = 60000\n",
    "print(f\"Length of training set is: {training_set_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuFrCLnOrA1J"
   },
   "source": [
    "Creates Train & Validation indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UuuR470ZrAah"
   },
   "outputs": [],
   "source": [
    "# Returning training and Validation indices \n",
    "def createTrainValIndices(training_set_length, p_train):\n",
    "\n",
    "  # Creating indices for our original training set \n",
    "  indices = np.linspace(0, training_set_length-1, num=training_set_length, dtype=int)\n",
    "  print(f\"Original in order indices: \\n{indices}\")\n",
    "  rand.shuffle(indices)\n",
    "  print(f\"Shuffled indices: \\n{indices}\")\n",
    "\n",
    "  # How to split the training and validation set\n",
    "  train_end_index = ma.floor(training_set_length*p_train)\n",
    "  print(f\"Training index start: {0}, Training index end: {train_end_index}\")\n",
    "  val_end_index = training_set_length\n",
    "  print(f\"Validation index start: {train_end_index}, Validation index end: {val_end_index}\")\n",
    "\n",
    "  # Slicing our original indices to form training and test indices\n",
    "  training_indices = indices[0:train_end_index]\n",
    "  val_indices = indices[train_end_index:val_end_index]\n",
    "\n",
    "  return training_indices, val_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2hS9M7msGeT"
   },
   "source": [
    "Our Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dNZ0BNHkk_JZ"
   },
   "outputs": [],
   "source": [
    "# img_file: the pickle file containing the images\n",
    "# label_file: the .csv file containing the labels\n",
    "# transform: We use it for normalizing images (see above)\n",
    "# idx: This is a binary vector that is useful for creating training and validation set.\n",
    "# It return only samples where idx is True\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "\n",
    "  # MyDataSet constructor which stores the pickled training images and its labels\n",
    "  def __init__(self, img_file, label_file, transform=None, idx = None, target_transform=None):\n",
    "    self.data = pickle.load( open(img_file, 'rb' ), encoding='bytes')\n",
    "    self.targets = np.genfromtxt(label_file, delimiter=',', skip_header=1)[:,1:]\n",
    "    if idx is not None:\n",
    "      self.targets = self.targets[idx]\n",
    "      self.data = self.data[idx]\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "  # returns the size of our data set\n",
    "  def __len__(self):\n",
    "    return len(self.targets)\n",
    "\n",
    "  # returns a specific image in the data set by index\n",
    "  def __getitem__(self, index):\n",
    "    img, target = self.data[index], int(self.targets[index])\n",
    "    img = Image.fromarray(img.astype('uint8'), mode='L')\n",
    "    if self.transform is not None:\n",
    "      img = self.transform(img)\n",
    "    return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuBMX4LsTpuf"
   },
   "source": [
    "# Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vz5r7HRLXaem",
    "outputId": "50a49f19-fbbb-400a-8343-fdb88e468a2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original in order indices: \n",
      "[    0     1     2 ... 59997 59998 59999]\n",
      "Shuffled indices: \n",
      "[31208 31258 26755 ... 15679 52646 48411]\n",
      "Training index start: 0, Training index end: 48000\n",
      "Validation index start: 48000, Validation index end: 60000\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "batch_size = 100\n",
    "training_indices, val_indices = createTrainValIndices(training_set_length, p_train=0.8)\n",
    "training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
    "                         transform=ImageTransforms, idx=training_indices, target_transform=target_tranform)\n",
    "trainingSetDataLoader = DataLoader(training_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k_aL_fP2Ts7Z",
    "outputId": "56da0774-7da3-4186-8a7f-f4e498a689e7"
   },
   "outputs": [],
   "source": [
    "# # Read a batch of data and their labels and display them\n",
    "# # Note that since data are transformed, they are between [-1,1]\n",
    "# imgs, labels = (next(iter(trainingSetDataLoader)))\n",
    "# imgs = np.squeeze(imgs)\n",
    "# for i in range(0,batch_size):\n",
    "#     plt.figure(figsize=(7,7))\n",
    "#     plt.imshow(imgs[i].cpu().numpy(),cmap='gray', vmin=-1, vmax=1) #.transpose()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mShhdcXsnBBl"
   },
   "source": [
    "# Test Set Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6n0bG2svnFM5"
   },
   "outputs": [],
   "source": [
    "# function to load our test set and apply the same image transformation pipeline\n",
    "# that we had done to our training batches\n",
    "def loadTestSet(test_file_path, transform=None):\n",
    "  with open(str(test_file_path), 'rb') as tf:\n",
    "    test_set = pickle.load(tf, encoding='bytes')\n",
    "    for i in range(0, len(test_set)):\n",
    "      test_set[i] = Image.fromarray(test_set[i].astype('uint8'), mode='L')\n",
    "\n",
    "  # applying the same transformations that was done to our images for the training set\n",
    "  test_set = transform(test_set)\n",
    "  test_set = test_set.reshape((test_set.shape[1],test_set.shape[2], test_set.shape[0]))\n",
    "  return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKw-uvuTo4pN",
    "outputId": "b3b02ff6-0fb0-4f11-ed14-5aebb7b93a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 64, 128])\n",
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]])\n"
     ]
    }
   ],
   "source": [
    "test_set = loadTestSet(test_set_pickle_path, ImageTransforms)\n",
    "print(test_set.shape)\n",
    "print(test_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdn5YixtOw12"
   },
   "source": [
    "# Convolution Neural Network Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAX3nEomCUju"
   },
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Y6JXAPgyCT23"
   },
   "outputs": [],
   "source": [
    "# @titledictionary holding a few common activation functions used in CNNs\n",
    "act_func_dict = {\n",
    "    'Relu': nn.ReLU(True), # defacto standard in deep learning these days\n",
    "    'Sig': nn.Sigmoid(), # may provide vanishing gradient problems in deep NNs\n",
    "    'Tanh': nn.Tanh(), # may provide vanishing gradient problems in deep NNs\n",
    "    'LeakyRelu': nn.LeakyReLU(), # slightly better than ReLU as it solves the problem of \"dead neurons\" in the network\n",
    "    'ELU': nn.ELU(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LON36fboPlqv"
   },
   "source": [
    "Modified VGG CNN with Dropout Regularization & Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XqnCiScIO5ZB"
   },
   "outputs": [],
   "source": [
    "# Our various VGG architecture specifications for each layers input/output sizes\n",
    "# M = MaxPool layer\n",
    "VGG_types = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M',512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256,'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64,'M', 128, 128,'M', 256, 256, 256, 256,'M', 512, 512, 512, 512,'M',512,512,512,512,'M'],\n",
    "}\n",
    "\n",
    "# For our fully connected layers\n",
    "VGG16_Structure_Lin = [4096, 9]\n",
    "\n",
    "# We will base our CNN on the VGG Nets \n",
    "# CNN is composed of two types of layers: \n",
    "# 1) Cov and Pooling - Feature Extraction \n",
    "# 2) Fully Connected Linear - Classification \n",
    "class Fashion_VGG_CNN(nn.Module):\n",
    "\n",
    "  # Constructor\n",
    "  # 9 way classification problem so num classes is 9\n",
    "  # in_channel is 1 becasue our images are gray scale\n",
    "  def __init__(self, in_channels = 1, num_classes = 9, dropout=0.15, vgg_type = \"VGG16\", act_func = act_func_dict['Relu']):\n",
    "    super(Fashion_VGG_CNN, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.conv_layers = self._create_cov_layers(self, VGG_types[vgg_type], act_func)\n",
    "    # This fcs represents the last 3 linear fully connected layers of the VGG16\n",
    "    self.fcs = nn.Sequential(\n",
    "        # so the input to the first linear fully connected layer would be\n",
    "        # H = 64/(2**5), W = 128/(2**5) then H*W*512\n",
    "        nn.Linear(in_features=(512*2*4), out_features=VGG16_Structure_Lin[0]),\n",
    "        act_func,\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features=VGG16_Structure_Lin[0], out_features=VGG16_Structure_Lin[0]),\n",
    "        act_func,\n",
    "        nn.Dropout(p=dropout),\n",
    "        nn.Linear(in_features=VGG16_Structure_Lin[0], out_features = num_classes)\n",
    "        )\n",
    "    \n",
    "  # Feed forwarding our images to find outputs \n",
    "  def forward(self, x):\n",
    "\n",
    "    # Sending each image through all of our convolution layers\n",
    "    x = self.conv_layers(x)\n",
    "    # After the last max pool layer we need to flatten image into a linear vector\n",
    "    x = x.view(x.size(0), -1)\n",
    "    # Now send the flattened vector into the last 3 fully connected linear layers\n",
    "    x = self.fcs(x)\n",
    "    m = nn.Softmax(dim=1)\n",
    "    return m(x)\n",
    "\n",
    "  # Creates the convolution layers for us for this CNN\n",
    "  @staticmethod\n",
    "  def _create_cov_layers(self, myArchitecture, act_func):\n",
    "    # image input channels for us its only 1 since it's gray scale\n",
    "    in_channels = self.in_channels\n",
    "    # define a list to hold the layers of the CNN\n",
    "    layers = []\n",
    "    # looping through the architecture to define our layers\n",
    "    for x in myArchitecture:\n",
    "      # if it is convolution layer \n",
    "      if (type(x) == int):\n",
    "        out_channels = x\n",
    "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                             kernel_size=(3,3), stride=(1,1), padding=(1,1)),\n",
    "                   nn.BatchNorm2d(x),\n",
    "                   act_func]\n",
    "        in_channels = x\n",
    "      # if it is a max pooling layer\n",
    "      elif (x == 'M'):\n",
    "        layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "    # Now return the block containing all these layers stacked one after another sequentially\n",
    "    return nn.Sequential(*layers)\n",
    "  \n",
    "  # Weight initializations\n",
    "  def reg_init_weights(self, m):\n",
    "    '''\n",
    "        regular model implementation of weight initialization\n",
    "    '''\n",
    "    if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
    "      nn.init.kaiming_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWNWAgoAT9mS",
    "outputId": "46ea7289-1287-4c24-b500-67fe36e95724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1386, 0.1047, 0.1258, 0.1044, 0.0958, 0.0961, 0.1154, 0.1222, 0.0971]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "model = Fashion_VGG_CNN(in_channels=1, num_classes=9, vgg_type= \"VGG11\")\n",
    "x = torch.randn(1,1,64,128)\n",
    "print(model(x))\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXK4ABc_VaAC"
   },
   "source": [
    "Simple Fashion CNN with Dropout Regularization & Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U4jXrjBmrybP"
   },
   "outputs": [],
   "source": [
    "# define a dictionary holding diff CNN dropout rates for each layer \n",
    "CNN_Dropout_rates = {\n",
    "    \"CNN_1\": [0.15],\n",
    "    \"CNN_2\": [0.15, 0.15],\n",
    "    \"CNN_3\": [0.15, 0.2, 0.3],\n",
    "    \"CNN_4\": [0.15, 0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# define a dictionary holding diff CNN configs\n",
    "CNN_Configs = {\n",
    "    \"CNN_1\": [32, 'M'],\n",
    "    \"CNN_2\": [32, 'M', 64, 'M'],\n",
    "    \"CNN_3\": [32, 'M', 64, 'M', 128, 'M'],\n",
    "    \"CNN_4\": [32, 'M', 64, 'M', 128, 'M', 256, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class Fashion_Simple_CNN(nn.Module):\n",
    "\n",
    "  # Constructor\n",
    "  # 9 way classification problem so num classes is 9\n",
    "  # in_channel is 1 becasue our images are gray scale\n",
    "  def __init__(self, in_channel = 1, num_classes = 9, cnn_type = \"CNN_3\", kernel_size = (3,3), \n",
    "               act_func = act_func_dict['Relu'], use_dropout_reg = True, use_batch_norm = True):\n",
    "    # super class constructor\n",
    "    super(Fashion_Simple_CNN, self).__init__() \n",
    "    # class variables \n",
    "    self.in_channel = in_channel\n",
    "    self.use_dropout_reg = use_dropout_reg\n",
    "    self.use_batch_norm = use_batch_norm\n",
    "    self.kernel_size = kernel_size\n",
    "    self.conv_layers = self._create_cov_layers(self, CNN_Configs[cnn_type], CNN_Dropout_rates[cnn_type], kernel_size, act_func)\n",
    "\n",
    "    # This fcs represents the last 2 linear fully connected layers of this simple CNN\n",
    "    if (cnn_type == \"CNN_3\"):\n",
    "      self.fcs = nn.Sequential(\n",
    "          # so the input to the first linear fully connected layer would be\n",
    "          # H = 64/(2**3), W = 128/(2**3) then H*W*128\n",
    "          nn.Linear(in_features=(128*8*16), out_features=1024),\n",
    "          nn.Dropout(p=0.25),\n",
    "          nn.Linear(in_features=1024, out_features=num_classes),\n",
    "          )\n",
    "    elif (cnn_type == \"CNN_4\"):\n",
    "      self.fcs = nn.Sequential(\n",
    "          # so the input to the first linear fully connected layer would be\n",
    "          # H = 64/(2**3), W = 128/(2**3) then H*W*256\n",
    "          nn.Linear(in_features=(256*4*8), out_features=1024),\n",
    "          nn.Dropout(p=0.25),\n",
    "          nn.Linear(in_features=1024, out_features=num_classes),\n",
    "          )\n",
    "\n",
    "    elif (cnn_type == \"CNN_2\"):\n",
    "      self.fcs = nn.Sequential(\n",
    "          # so the input to the first linear fully connected layer would be\n",
    "          # H = 64/(2**2), W = 128/(2**2) then H*W*64\n",
    "          nn.Linear(in_features=(64*16*32), out_features=1024),\n",
    "          nn.Dropout(p=0.15),\n",
    "          nn.Linear(in_features=1024, out_features=num_classes)\n",
    "          )\n",
    "\n",
    "    elif (cnn_type == \"CNN_1\"):\n",
    "      self.fcs = nn.Sequential(\n",
    "          # so the input to the first linear fully connected layer would be\n",
    "          # H = 64/(2**1), W = 128/(2**1) then H*W*32\n",
    "          nn.Linear(in_features=(32*32*64), out_features=1024),\n",
    "          nn.Dropout(p=0.25),\n",
    "          nn.Linear(in_features=1024, out_features=num_classes)\n",
    "          )\n",
    "\n",
    "\n",
    "  # feed forward our image data to compute y\n",
    "  def forward(self, x):\n",
    "    # Sending each image through all of our convolution layers\n",
    "    x = self.conv_layers(x)\n",
    "    # After the last max pool layer we need to flatten image into a linear vector\n",
    "    x = x.view(x.size(0), -1)\n",
    "    # Now send the flattened vector into the last 2 fully connected linear layers\n",
    "    # print(x.shape)\n",
    "    x = self.fcs(x)\n",
    "    # We should put an appropriate activation for the output layer.\n",
    "    m = nn.Softmax(dim=1)\n",
    "    return m(x)\n",
    "  \n",
    "  # Creates the convolution layers for us for this CNN\n",
    "  @staticmethod\n",
    "  def _create_cov_layers(self, conv_architecture, conv_dropout_rates, kernel_size, act_func):\n",
    "    \n",
    "    # To keep track of drop out rates used at each conv layer\n",
    "    index = 0\n",
    "    # Define in channel value\n",
    "    in_channel = self.in_channel\n",
    "    # List to hold our layers\n",
    "    layers = []\n",
    "\n",
    "    # Loop to go through our CNN output size specification\n",
    "    for x in conv_architecture:\n",
    "\n",
    "      # If it's a conv layer\n",
    "      if (type(x) == int):\n",
    "        out_channels = x\n",
    "        if (self.use_batch_norm):\n",
    "          layers += [nn.Conv2d(in_channels=in_channel, out_channels=out_channels, \n",
    "                             kernel_size=kernel_size, stride=(1,1), padding=(1,1)),\n",
    "                     nn.BatchNorm2d(x), act_func]\n",
    "        else:\n",
    "          layers += [nn.Conv2d(in_channels=in_channel, out_channels=out_channels, \n",
    "                             kernel_size=kernel_size, stride=(1,1), padding=(1,1)), act_func]\n",
    "        in_channel = x\n",
    "      # if it is a max pooling layer\n",
    "      elif (x == 'M'):\n",
    "        if (self.use_dropout_reg):\n",
    "          layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)), nn.Dropout(p=conv_dropout_rates[index])]\n",
    "          index += 1\n",
    "        else:\n",
    "          layers += [nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))]\n",
    "\n",
    "    # Now return the block containing all these layers stacked one after another sequentially\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  # Weight initializations\n",
    "  def reg_init_weights(self, m):\n",
    "    '''\n",
    "        regular model implementation of weight initialization\n",
    "    '''\n",
    "    if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
    "      nn.init.kaiming_normal_(m.weight)\n",
    "      m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVOhODtr90aN"
   },
   "source": [
    "Modified CNN From Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oW1Xry8A94Gt"
   },
   "outputs": [],
   "source": [
    "class CNN_Tutorial(nn.Module):\n",
    "    # This part defines the layers\n",
    "    def __init__(self, in_channel = 1, num_classes = 9, first_kernel = 3, sec_kernel = 3,  \n",
    "                           act_func = act_func_dict['Relu'], p_dropout=0.15):\n",
    "        super(CNN_Tutorial, self).__init__()\n",
    "        \n",
    "        # Calculate the input size of the first linear layer\n",
    "        first_kernel = first_kernel\n",
    "        cov1_param1 = int((64-first_kernel+1)/2)\n",
    "        cov1_param2 = int((128-first_kernel+1)/2)\n",
    "        sec_kernel = sec_kernel\n",
    "        cov2_param1 = int((cov1_param1-sec_kernel+1)/2)\n",
    "        cov2_param2 = int((cov1_param2-sec_kernel+1)/2)\n",
    "        fc1_input_size = 20 * cov2_param1 * cov2_param2\n",
    "        print(f\"The input size of my first linear layer is: {fc1_input_size}\")\n",
    "\n",
    "        # conv layer 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channel, out_channels=10, kernel_size=first_kernel),\n",
    "            nn.BatchNorm2d(10),\n",
    "            act_func,\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        # conv layer 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=sec_kernel),\n",
    "            nn.BatchNorm2d(20),\n",
    "            act_func,\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        )\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(fc1_input_size, 600),\n",
    "            act_func,\n",
    "            nn.Dropout(p=p_dropout),\n",
    "            nn.Linear(600, 120),\n",
    "            act_func,\n",
    "            nn.Linear(120, num_classes)\n",
    "        )\n",
    "\n",
    "    # And this part defines the way they are connected to each other\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fcs(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "    # Weight initializations\n",
    "    def reg_init_weights(self, m):\n",
    "      '''\n",
    "          regular model implementation of weight initialization\n",
    "      '''\n",
    "      if (type(m) == nn.Conv2d or type(m) == nn.Linear):\n",
    "          nn.init.kaiming_normal_(m.weight)\n",
    "          m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEXCKXrQ4IAQ"
   },
   "source": [
    "CNN with Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aT0XI_goqdD3"
   },
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, channel_sizes, layers, batch_norm, dropout, num_classes):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        modules = []\n",
    "        for block_idx in range(0, len(channel_sizes) -1):\n",
    "            modules.append(nn.Conv2d(channel_sizes[block_idx], channel_sizes[block_idx+1], 3, padding=1, bias=False))\n",
    "            if batch_norm:\n",
    "                modules.append(nn.BatchNorm2d(channel_sizes[block_idx+1]))\n",
    "            modules.append(nn.ReLU(True))\n",
    "            if dropout is not None:\n",
    "                modules.append(nn.Dropout2d(dropout, inplace=False))\n",
    "            if layers > 1:\n",
    "                for layer in range(layers - 1):\n",
    "                    modules.append(nn.Conv2d(channel_sizes[block_idx+1], channel_sizes[block_idx+1], 3, padding=1, bias=False))\n",
    "                    if batch_norm:\n",
    "                        modules.append(nn.BatchNorm2d(channel_sizes[block_idx+1]))\n",
    "                    modules.append(nn.ReLU(True))\n",
    "                    if dropout is not None:\n",
    "                        modules.append(nn.Dropout2d(dropout, inplace=False))\n",
    "            \n",
    "            if block_idx + 1  != len(channel_sizes) - 1:\n",
    "                modules.append(nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.cnn_core = nn.Sequential(*modules)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(channel_sizes[-1], num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_core(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        m = nn.Softmax(dim=1)\n",
    "        return m(x)\n",
    "    \n",
    "    # Weight initializations\n",
    "    def reg_init_weights(self, m):\n",
    "      '''\n",
    "          regular model implementation of weight initialization\n",
    "      '''\n",
    "      if (type(m) == nn.Conv3d or type(m) == nn.Linear):\n",
    "          nn.init.kaiming_normal_(m.weight)\n",
    "          m.bias.data.fill_(0.01)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQzKV6-_9Zhj"
   },
   "source": [
    "# Loss Functions & Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "we_9_dHJ94jt"
   },
   "source": [
    "Dictionary of Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "G1n-f2Jt9g1l"
   },
   "outputs": [],
   "source": [
    "# function returning our desired loss function\n",
    "def selectLoss(key):\n",
    "  loss_func_dict = {\n",
    "      \"CEL\": nn.CrossEntropyLoss(),\n",
    "      \"KLDL\": nn.KLDivLoss(),\n",
    "      \"NLL\": nn.NLLLoss(),\n",
    "      \"MSE\": nn.MSELoss(),\n",
    "  }\n",
    "  return loss_func_dict.get(key,\"Invalid loss function!\") \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7AeJL58BDzB"
   },
   "source": [
    "Dictionary of Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JBhVe4mPBJh8"
   },
   "outputs": [],
   "source": [
    "# function returns our desired optimizer\n",
    "def selectOptimizer(key, model, lr, momentum):\n",
    "  optimizers_dict = {\n",
    "      \"SGD\": optim.SGD(model.parameters(), lr=lr, momentum=momentum),\n",
    "      \"Adam\": optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08),\n",
    "      \"RMS\": optim.RMSprop(model.parameters(), lr=lr, momentum=momentum),\n",
    "      \"AdaG\": optim.Adagrad(model.parameters(), lr=lr),\n",
    "      \"AdaD\": optim.Adadelta(model.parameters(), lr=lr)\n",
    "  }\n",
    "  return optimizers_dict.get(key,\"Invalid optimizer!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa2u381rFFo1"
   },
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "athqOc-ZYiw-"
   },
   "source": [
    "Simple CNN with Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wstUkvJvYiGn"
   },
   "outputs": [],
   "source": [
    "# Basic_CNN = BasicCNN(channel_sizes=cnn_config, layers = 1, batch_norm=True, dropout=0.15, num_classes=9)\n",
    "# Basic_CNN.apply(Basic_CNN.reg_init_weights)\n",
    "# print(Basic_CNN)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYP1sO1CFcmV"
   },
   "source": [
    "VGG Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "SfPsoSQCFOJs"
   },
   "outputs": [],
   "source": [
    "vgg_CNN = Fashion_VGG_CNN(in_channels=1, num_classes=9, vgg_type=\"VGG11\", act_func = act_func_dict['Relu'])\n",
    "# print(vgg_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CzgiDMyFPlU"
   },
   "source": [
    "Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mUp2FE-DFPBR"
   },
   "outputs": [],
   "source": [
    "simple_CNN = Fashion_Simple_CNN(in_channel = 1, num_classes = 9, cnn_type = \"CNN_3\", kernel_size = (3,3), \n",
    "                           act_func = act_func_dict['Relu'], use_dropout_reg = True, use_batch_norm = True)\n",
    "# print(simple_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsCtDJFZf_hc"
   },
   "source": [
    "# Hyper-parameters & Tunning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "WdV69AaQqK3U"
   },
   "outputs": [],
   "source": [
    "# Define all the hyperparameters\n",
    "model_name = \"./VGG19_model.tar\"\n",
    "vgg_type = \"VGG19\"\n",
    "cnn_config = [1,64,128,256]\n",
    "p_train = 0.80\n",
    "step_size = 3 # every number of epochs decrease the learning rate \n",
    "gamma = 0.1 # decrease leanring rate by gamma /10 every number of epochs or so\n",
    "lr = 1e-5\n",
    "batch_size = 128\n",
    "print_results_every = 20\n",
    "epoch_start = 1\n",
    "num_epochs = 200\n",
    "momentum = 0.5\n",
    "loss_key = \"CEL\"\n",
    "optimizer_key = \"Adam\"\n",
    "act_func = act_func_dict['Relu']\n",
    "kernel_size = (3,3)\n",
    "stride = (1,1)\n",
    "first_kernel = 5\n",
    "sec_kernel = 5\n",
    "p_dropout = 0.5\n",
    "num_layers = 1\n",
    "use_batch_norm = True\n",
    "small_train_size = 500\n",
    "small_val_size = 100\n",
    "training_set_length = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAH9IYBXT5Zh"
   },
   "source": [
    "# TorchVision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9SwL9Cs-T8rI"
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=False)\n",
    "vgg11_bn = models.vgg11_bn(pretrained=False)\n",
    "vgg13_bn = models.vgg13_bn(pretrained=False)\n",
    "vgg16_bn = models.vgg16_bn(pretrained=False)\n",
    "vgg19_bn = models.vgg19_bn(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lsZAIGKLoUR"
   },
   "source": [
    "# CNN Training & Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "lLYs1Dd1Og9w"
   },
   "outputs": [],
   "source": [
    "# function used to evaluate our validation accuracy on during training \n",
    "def evaluate(model, validation_loader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for val_data, val_labels in validation_loader:\n",
    "      val_data, val_labels = val_data.to(DEVICE), val_labels.to(DEVICE)\n",
    "      val_outputs = model(val_data)\n",
    "      _,predicted = torch.max(val_outputs.data, 1)\n",
    "      total += val_labels.size(0)\n",
    "      correct += (predicted == val_labels).sum()\n",
    "    current_val_acc = ((float(correct)/float(total))*100)\n",
    "  return current_val_acc, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "AxVbAxyh0uUd"
   },
   "outputs": [],
   "source": [
    "# create small validaiton and training indices\n",
    "def createSmallTrainValIndicies(small_train_size, small_val_size, training_set_length):\n",
    "  # Creating indices for our original training set \n",
    "  indices = np.linspace(0, training_set_length-1, num=training_set_length, dtype=int)\n",
    "  # print(f\"Original in order indices: \\n{indices}\")\n",
    "  rand.shuffle(indices)\n",
    "  # print(f\"Shuffled indices: \\n{indices}\")\n",
    "  half_index = ma.floor(len(indices)/2)\n",
    "  small_train_indices = indices[0:small_train_size]\n",
    "  small_val_indices = indices[half_index:half_index + small_val_size]\n",
    "  return small_train_indices, small_val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "LfBFzeA7LyL8"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def trainCNN(epoch, model, optimizer, loss_function, \n",
    "             validation_loader, train_loader, \n",
    "             b, train_losses, train_counter, model_name):\n",
    "  \n",
    "  # Defining the running accuracy of training\n",
    "  train_correct = 0\n",
    "  train_total = 0\n",
    "  for batch_id, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    # our model is now in training phase \n",
    "    # ensure's our model will use batch norm layers and dropout layers for training\n",
    "    model.train()\n",
    "\n",
    "    # Initializing grad to 0 to ensure there is no mixing of graidents among batches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # move a batch of images and it's labels into GPU\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_labels = batch_labels.to(DEVICE)\n",
    "    # print(torch.unique(batch_labels))\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(batch_data)\n",
    "\n",
    "    # Adding to the running sum of accuracy\n",
    "    _,pred = torch.max(outputs.data, 1)\n",
    "    train_correct += (pred == batch_labels).sum()\n",
    "    train_total += batch_data.size(0)\n",
    "    \n",
    "    # Calculate loss\n",
    "    # print(torch.unique(batch_labels))\n",
    "    current_loss = loss_function(outputs, batch_labels)\n",
    "\n",
    "    # Propagate error backwards\n",
    "    current_loss.backward()\n",
    "\n",
    "    # Optimize our model parameters and update scheduler\n",
    "    optimizer.step()\n",
    "\n",
    "    # Printing out our training and validation progress every so often\n",
    "    if (batch_id % b == 0):\n",
    "      \n",
    "      # finding out the training accuracy every however many batches \n",
    "      current_train_acc = ((float(train_correct)/float(train_total))*100)\n",
    "\n",
    "      # finding validation accuracy\n",
    "      current_val_acc, val_correct, val_total = evaluate(model, validation_loader)\n",
    "\n",
    "      # printing our results \n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_id * len(batch_data), len(train_loader.dataset),\n",
    "          100. * batch_id / len(train_loader), current_loss.item()))\n",
    "      print(f\"Train Epoch: {epoch} Validation_Accuracy: {current_val_acc}%, Ratio: {val_correct}/{val_total}\")\n",
    "      print(f\"Train Epoch: {epoch} Training_Accuracy: {current_train_acc}%, Ratio: {train_correct}/{train_total}\\n\")\n",
    "\n",
    "\n",
    "      train_losses.append(current_loss.item())\n",
    "      train_counter.append((batch_id*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      \n",
    "      # saving our model, optimizer, and training counter/losses lists\n",
    "      torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_losses,\n",
    "            'train_counter': train_counter\n",
    "            }, str(model_name), _use_new_zipfile_serialization=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAwaVFE3nC0K"
   },
   "source": [
    "# CNN Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJtTTrgVuE9q"
   },
   "source": [
    "Mini-batches for Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AigbMFKVqK-x",
    "outputId": "d7c1f6ef-a73c-4252-8e09-c632c5981656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500, Validation size: 100\n"
     ]
    }
   ],
   "source": [
    "# create small train and val indices for slicing our data set\n",
    "small_train_indices, small_val_indices = createSmallTrainValIndicies(small_train_size, small_val_size, training_set_length)\n",
    "print(f\"Train size: {len(small_train_indices)}, Validation size: {len(small_val_indices)}\")\n",
    "\n",
    "# Create small train and small val dataset\n",
    "small_dataset = MyDataSet(training_set_pickle_path, training_labels_path, \n",
    "                         transform=ImageTransforms, idx=small_train_indices, target_transform=target_tranform)\n",
    "small_dataset.targets = small_dataset.target_transform(small_dataset.targets)\n",
    "small_valset = MyDataSet(training_set_pickle_path, training_labels_path, \n",
    "                         transform=ImageTransforms, idx=small_val_indices, target_transform=target_tranform)\n",
    "small_valset.targets = small_valset.target_transform(small_valset.targets)\n",
    "\n",
    "# Create small train and validation dataloaaders\n",
    "small_train_loader = DataLoader(small_dataset, batch_size=batch_size, shuffle=True)\n",
    "small_val_loader = DataLoader(small_valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqMwyPblsaAa"
   },
   "source": [
    "Full Training & Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AgOBSfYp8ce",
    "outputId": "cc7e15f9-253a-42e2-e663-109d797fb6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original in order indices: \n",
      "[    0     1     2 ... 59997 59998 59999]\n",
      "Shuffled indices: \n",
      "[58464 51282 53809 ... 58853 47203 12277]\n",
      "Training index start: 0, Training index end: 48000\n",
      "Validation index start: 48000, Validation index end: 60000\n",
      "[[8]\n",
      " [3]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [8]\n",
      " [2]\n",
      " [6]\n",
      " [6]\n",
      " [2]]\n",
      "My training set shape is: (48000, 64, 128)\n",
      "My training set labels shape is: (48000, 1)\n",
      "[[0]\n",
      " [5]\n",
      " [7]\n",
      " [5]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [8]\n",
      " [3]\n",
      " [0]]\n",
      "My validation set shape is: (12000, 64, 128)\n",
      "My validation set labels shape is: (12000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Creating training and val indices so our data set class can chop them up appropriately \n",
    "training_indices, val_indices = createTrainValIndices(training_set_length, p_train=0.8)\n",
    "\n",
    "# Create training dataset \n",
    "training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
    "                         transform=ImageTransforms, idx=training_indices, target_transform=target_tranform)\n",
    "training_set.targets = training_set.target_transform(training_set.targets).astype(int)\n",
    "print(training_set.targets[0:10,:])\n",
    "print(f\"My training set shape is: {training_set.data.shape}\")\n",
    "print(f\"My training set labels shape is: {training_set.targets.shape}\")\n",
    "\n",
    "# Create validation dataset \n",
    "validation_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
    "                           transform=ImageTransforms, idx=val_indices, target_transform=target_tranform)\n",
    "validation_set.targets = validation_set.target_transform(validation_set.targets)\n",
    "validation_set.targets = validation_set.targets.astype(int)\n",
    "print(validation_set.targets[0:10,:])\n",
    "print(f\"My validation set shape is: {validation_set.data.shape}\")\n",
    "print(f\"My validation set labels shape is: {validation_set.targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE0k-DmJtCB0"
   },
   "source": [
    "Create Training & Validation Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Okfd6Y_YtLI2"
   },
   "outputs": [],
   "source": [
    "# Create training and validation loaders\n",
    "trainingSetDataLoader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validationSetDataLoader = DataLoader(validation_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJzSwZo8wjCi"
   },
   "source": [
    "Create different CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbOY-dx5qLFV",
    "outputId": "01295ef2-b885-4a79-a173-d4fe3a9269bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input size of my first linear layer is: 7540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN_Tutorial(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=7540, out_features=600, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=600, out_features=120, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=120, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tutorial CNN\n",
    "CNN_Tutorial = CNN_Tutorial(in_channel=1, num_classes=9, \n",
    "                            first_kernel=first_kernel, \n",
    "                            sec_kernel=sec_kernel, act_func=act_func,p_dropout=p_dropout)\n",
    "CNN_Tutorial.apply(CNN_Tutorial.reg_init_weights)\n",
    "# print(CNN_Tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQGA9FP3EWTd",
    "outputId": "1010560e-9f6d-48eb-c147-cbed6fd27002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicCNN(\n",
       "  (cnn_core): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout2d(p=0, inplace=False)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout2d(p=0, inplace=False)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Dropout2d(p=0, inplace=False)\n",
       "  )\n",
       "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (linear): Linear(in_features=256, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic CNN\n",
    "cnn_basic = BasicCNN(channel_sizes=cnn_config, layers = 1, batch_norm=True, dropout=0, num_classes=9)\n",
    "cnn_basic.apply(cnn_basic.reg_init_weights)\n",
    "# print(cnn_basic)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Lv5b5mhMnMI",
    "outputId": "f42bf78e-a55f-43d1-e1bf-15c651dcf076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fashion_Simple_CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.15, inplace=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.15, inplace=False)\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (1): Dropout(p=0.15, inplace=False)\n",
       "    (2): Linear(in_features=1024, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_CNN = Fashion_Simple_CNN(in_channel = 1, num_classes = 9, cnn_type = \"CNN_2\", kernel_size = (3,3), \n",
    "                           act_func = act_func_dict['Relu'], use_dropout_reg = True, use_batch_norm = True)\n",
    "simple_CNN.apply(simple_CNN.reg_init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Nur2lJQ2Fpv",
    "outputId": "45ac20fa-1d1d-4c95-d390-6daf516b2b29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fashion_VGG_CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fcs): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VGG_CNN = Fashion_VGG_CNN(in_channels=1, num_classes=9, dropout=p_dropout, vgg_type=vgg_type, act_func = act_func)\n",
    "VGG_CNN.apply(VGG_CNN.reg_init_weights)\n",
    "# print(VGG_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9y0BQjhShQFq"
   },
   "source": [
    "Set-up Loss Function and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0hiYnNoeNQu",
    "outputId": "f4822003-d3f6-467a-abaf-4346c7a1bce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion_VGG_CNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fcs): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.15, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.15, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a net\n",
    "model = vgg_CNN\n",
    "print(model)\n",
    "\n",
    "# moving our CNN model into GPU memory\n",
    "if USE_CUDA:\n",
    "  model = model.to(DEVICE)\n",
    "\n",
    "# create loss function\n",
    "loss_function = selectLoss(key=loss_key)\n",
    "\n",
    "# create optimizer and step scheduler\n",
    "optimizer = selectOptimizer(key=optimizer_key, model=model, lr=lr, momentum=momentum)\n",
    "# step_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = step_size, gamma=gamma, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "5y9byLvkdh34"
   },
   "outputs": [],
   "source": [
    "# define lists to hold our training loss and iterations\n",
    "train_losses = []\n",
    "train_counter = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0agmfN_8nFvw"
   },
   "source": [
    "# GridSearchCV with Skorch For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaxB0P5PnM2y",
    "outputId": "88cfdb61-88e9-4c93-adef-b8e2134444a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (48000, 1, 64, 128), Validation set shape: (12000, 1, 64, 128)\n",
      "Training labels shape: (48000,), Validation labels shape: (12000,)\n"
     ]
    }
   ],
   "source": [
    "# reshaping data \n",
    "ts = training_set.data.reshape(-1, 1, 64, 128).astype('float32')\n",
    "ts_labels = training_set.targets.reshape((training_set.targets.shape[0],)).astype('int64')\n",
    "vs = validation_set.data.reshape(-1, 1, 64, 128).astype('float32')\n",
    "vs_labels = validation_set.targets.reshape((validation_set.targets.shape[0],)).astype('int64')\n",
    "print(f\"Training set shape: {ts.shape}, Validation set shape: {vs.shape}\")\n",
    "print(f\"Training labels shape: {ts_labels.shape}, Validation labels shape: {vs_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4ZtGSwmdnNK9"
   },
   "outputs": [],
   "source": [
    "# fixed random seed and cuda random seed \n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "# in_channels = 1, num_classes = 9, dropout=0.15, vgg_type = \"VGG16\", act_func = act_func_dict['Relu']\n",
    "\n",
    "# wrapping my own cnn class in skorch cnn\n",
    "cnn = NeuralNetClassifier(\n",
    "    module = Fashion_VGG_CNN,\n",
    "    module__in_channels = 1,\n",
    "    module__num_classes = 9,\n",
    "    module__dropout = p_dropout,\n",
    "    module__vgg_type = vgg_type,\n",
    "    module__act_func = act_func,\n",
    "    max_epochs=5,\n",
    "    lr=1e-5,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    device=DEVICE,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jR6m2DQ_nNPK"
   },
   "outputs": [],
   "source": [
    "# # training with skorch\n",
    "# cnn.fit(ts, ts_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlvzeNpanNS9",
    "outputId": "0b3cd9eb-9ff4-474a-cd80-423176916979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (48000, 1, 64, 128), Training Labels shape: (48000,)\n"
     ]
    }
   ],
   "source": [
    "# full data set of my training\n",
    "full_training_set = MyDataSet(training_set_pickle_path, training_labels_path, \n",
    "                         transform=ImageTransforms, idx=training_indices, target_transform=target_tranform)\n",
    "\n",
    "full_training_set.targets = full_training_set.target_transform(full_training_set.targets)\n",
    "gs_ts = full_training_set.data.reshape(-1, 1, 64, 128).astype('float32')\n",
    "gs_tl = full_training_set.targets.reshape((full_training_set.targets.shape[0],)).astype('int64')\n",
    "print(f\"Training set shape: {gs_ts.shape}, Training Labels shape: {gs_tl.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RQqZ9kCnNYL",
    "outputId": "ca5014df-7f5a-4aa7-9939-f8447576bc85"
   },
   "outputs": [],
   "source": [
    "# # doing gridsearch with skortch \n",
    "# params = {\n",
    "#     'lr': [1e-5, 5e-7],\n",
    "#     'max_epochs': [5, 10],\n",
    "#     'module__dropout': [0.25, 0.5],\n",
    "# }\n",
    "# gs = GridSearchCV(cnn, params, refit=False, cv=3, scoring='accuracy', verbose=True)\n",
    "# gs.fit(gs_ts, gs_tl)\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GS1tc8nVyO95"
   },
   "source": [
    "# Reload Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "cD0b9w3hEvBP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded from checkpoint successfully!\n"
     ]
    }
   ],
   "source": [
    "# if load is true meaning we are starting from where we left off after we stopped training\n",
    "load = True\n",
    "if load:\n",
    "  if USE_CUDA == 0:\n",
    "    # load checkpoint dictionary into CPU\n",
    "    checkpoint = torch.load(str(model_name), map_location=torch.device('cpu'))\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_loss']\n",
    "    train_counter = checkpoint['train_counter']\n",
    "  else:\n",
    "    # load checkpoint dictionary into GPU\n",
    "    checkpoint = torch.load(model_name, map_location=DEVICE)\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    train_losses = checkpoint['train_loss']\n",
    "    train_counter = checkpoint['train_counter']\n",
    "  print(f\"Reloaded from checkpoint successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nuHa2fSyfxM"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ZVmXTewWev0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 1.939738\n",
      "Train Epoch: 1 Validation_Accuracy: 51.4%, Ratio: 6168/12000\n",
      "Train Epoch: 1 Training_Accuracy: 41.40625%, Ratio: 53/128\n",
      "\n",
      "Train Epoch: 1 [2560/48000 (5%)]\tLoss: 1.795103\n",
      "Train Epoch: 1 Validation_Accuracy: 51.65%, Ratio: 6198/12000\n",
      "Train Epoch: 1 Training_Accuracy: 52.269345238095234%, Ratio: 1405/2688\n",
      "\n",
      "Train Epoch: 1 [5120/48000 (11%)]\tLoss: 1.802510\n",
      "Train Epoch: 1 Validation_Accuracy: 52.59166666666667%, Ratio: 6311/12000\n",
      "Train Epoch: 1 Training_Accuracy: 53.105945121951216%, Ratio: 2787/5248\n",
      "\n",
      "Train Epoch: 1 [7680/48000 (16%)]\tLoss: 1.848431\n",
      "Train Epoch: 1 Validation_Accuracy: 52.94166666666666%, Ratio: 6353/12000\n",
      "Train Epoch: 1 Training_Accuracy: 53.41956967213115%, Ratio: 4171/7808\n",
      "\n",
      "Train Epoch: 1 [10240/48000 (21%)]\tLoss: 1.840503\n",
      "Train Epoch: 1 Validation_Accuracy: 52.708333333333336%, Ratio: 6325/12000\n",
      "Train Epoch: 1 Training_Accuracy: 53.607253086419746%, Ratio: 5558/10368\n",
      "\n",
      "Train Epoch: 1 [12800/48000 (27%)]\tLoss: 1.811185\n",
      "Train Epoch: 1 Validation_Accuracy: 53.583333333333336%, Ratio: 6430/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.1769801980198%, Ratio: 7004/12928\n",
      "\n",
      "Train Epoch: 1 [15360/48000 (32%)]\tLoss: 1.880446\n",
      "Train Epoch: 1 Validation_Accuracy: 53.39166666666667%, Ratio: 6407/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.132231404958674%, Ratio: 8384/15488\n",
      "\n",
      "Train Epoch: 1 [17920/48000 (37%)]\tLoss: 1.816926\n",
      "Train Epoch: 1 Validation_Accuracy: 53.675%, Ratio: 6441/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.18328900709219%, Ratio: 9779/18048\n",
      "\n",
      "Train Epoch: 1 [20480/48000 (43%)]\tLoss: 1.930336\n",
      "Train Epoch: 1 Validation_Accuracy: 53.725%, Ratio: 6447/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.192546583850934%, Ratio: 11168/20608\n",
      "\n",
      "Train Epoch: 1 [23040/48000 (48%)]\tLoss: 1.779256\n",
      "Train Epoch: 1 Validation_Accuracy: 53.800000000000004%, Ratio: 6456/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.39399171270718%, Ratio: 12602/23168\n",
      "\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 1.832705\n",
      "Train Epoch: 1 Validation_Accuracy: 54.233333333333334%, Ratio: 6508/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.25995024875622%, Ratio: 13960/25728\n",
      "\n",
      "Train Epoch: 1 [28160/48000 (59%)]\tLoss: 1.816257\n",
      "Train Epoch: 1 Validation_Accuracy: 54.041666666666664%, Ratio: 6485/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.43651018099548%, Ratio: 15399/28288\n",
      "\n",
      "Train Epoch: 1 [30720/48000 (64%)]\tLoss: 1.835367\n",
      "Train Epoch: 1 Validation_Accuracy: 54.608333333333334%, Ratio: 6553/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.45733921161826%, Ratio: 16799/30848\n",
      "\n",
      "Train Epoch: 1 [33280/48000 (69%)]\tLoss: 1.877998\n",
      "Train Epoch: 1 Validation_Accuracy: 54.53333333333333%, Ratio: 6544/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.48694923371647%, Ratio: 18203/33408\n",
      "\n",
      "Train Epoch: 1 [35840/48000 (75%)]\tLoss: 1.836791\n",
      "Train Epoch: 1 Validation_Accuracy: 54.61666666666667%, Ratio: 6554/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.62911476868327%, Ratio: 19649/35968\n",
      "\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 1.759051\n",
      "Train Epoch: 1 Validation_Accuracy: 54.86666666666666%, Ratio: 6584/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.66414036544851%, Ratio: 21061/38528\n",
      "\n",
      "Train Epoch: 1 [40960/48000 (85%)]\tLoss: 1.856891\n",
      "Train Epoch: 1 Validation_Accuracy: 54.675%, Ratio: 6561/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.706970404984425%, Ratio: 22478/41088\n",
      "\n",
      "Train Epoch: 1 [43520/48000 (91%)]\tLoss: 1.822114\n",
      "Train Epoch: 1 Validation_Accuracy: 54.81666666666667%, Ratio: 6578/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.78372434017595%, Ratio: 23912/43648\n",
      "\n",
      "Train Epoch: 1 [46080/48000 (96%)]\tLoss: 1.871644\n",
      "Train Epoch: 1 Validation_Accuracy: 54.69166666666667%, Ratio: 6563/12000\n",
      "Train Epoch: 1 Training_Accuracy: 54.8498095567867%, Ratio: 25345/46208\n",
      "\n",
      "\n",
      "Run time per epoch: 14238.074902057648 (s) = 237.30124836762747 (mins)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0%)]\tLoss: 1.862282\n",
      "Train Epoch: 2 Validation_Accuracy: 55.041666666666664%, Ratio: 6605/12000\n",
      "Train Epoch: 2 Training_Accuracy: 50.78125%, Ratio: 65/128\n",
      "\n",
      "Train Epoch: 2 [2560/48000 (5%)]\tLoss: 1.818729\n",
      "Train Epoch: 2 Validation_Accuracy: 55.21666666666667%, Ratio: 6626/12000\n",
      "Train Epoch: 2 Training_Accuracy: 55.989583333333336%, Ratio: 1505/2688\n",
      "\n",
      "Train Epoch: 2 [5120/48000 (11%)]\tLoss: 1.839306\n",
      "Train Epoch: 2 Validation_Accuracy: 55.358333333333334%, Ratio: 6643/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.32621951219512%, Ratio: 2956/5248\n",
      "\n",
      "Train Epoch: 2 [7680/48000 (16%)]\tLoss: 1.786441\n",
      "Train Epoch: 2 Validation_Accuracy: 55.08333333333333%, Ratio: 6610/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.50614754098361%, Ratio: 4412/7808\n",
      "\n",
      "Train Epoch: 2 [10240/48000 (21%)]\tLoss: 1.829397\n",
      "Train Epoch: 2 Validation_Accuracy: 55.30833333333334%, Ratio: 6637/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.23070987654321%, Ratio: 5830/10368\n",
      "\n",
      "Train Epoch: 2 [12800/48000 (27%)]\tLoss: 1.766298\n",
      "Train Epoch: 2 Validation_Accuracy: 55.45833333333333%, Ratio: 6655/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.11076732673267%, Ratio: 7254/12928\n",
      "\n",
      "Train Epoch: 2 [15360/48000 (32%)]\tLoss: 1.768606\n",
      "Train Epoch: 2 Validation_Accuracy: 55.233333333333334%, Ratio: 6628/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.37913223140496%, Ratio: 8732/15488\n",
      "\n",
      "Train Epoch: 2 [17920/48000 (37%)]\tLoss: 1.768983\n",
      "Train Epoch: 2 Validation_Accuracy: 55.90833333333334%, Ratio: 6709/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.371897163120565%, Ratio: 10174/18048\n",
      "\n",
      "Train Epoch: 2 [20480/48000 (43%)]\tLoss: 1.748055\n",
      "Train Epoch: 2 Validation_Accuracy: 55.400000000000006%, Ratio: 6648/12000\n",
      "Train Epoch: 2 Training_Accuracy: 56.61393633540372%, Ratio: 11667/20608\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1ee60e1a8820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m#   optimizer = selectOptimizer(key=optimizer_key, model=model, lr=lr, momentum=momentum)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   trainCNN(epoch, model, optimizer, loss_function, validationSetDataLoader, trainingSetDataLoader, \n\u001b[0m\u001b[1;32m     11\u001b[0m            b=print_results_every, train_losses=train_losses, train_counter=train_counter, model_name=model_name)\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# if (epoch % step_size == 0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-6f42fb0ae860>\u001b[0m in \u001b[0;36mtrainCNN\u001b[0;34m(epoch, model, optimizer, loss_function, validation_loader, train_loader, b, train_losses, train_counter, model_name)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0;31m# finding validation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m       \u001b[0mcurrent_val_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;31m# printing our results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-9914c206c23d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, validation_loader)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mval_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b4a2060b30f5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Sending each image through all of our convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m# After the last max pool layer we need to flatten image into a linear vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 415\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training converges at around 6 epochs for the tutorial cnn\n",
    "# training for a number of epochs\n",
    "last_epoch_num = 0\n",
    "for epoch in range(epoch_start, num_epochs):\n",
    "  # if (epoch % step_size == 0):\n",
    "  #   lr = lr * 0.1\n",
    "  #   print(f\"Learning rate set to: {lr}\")\n",
    "  #   optimizer = selectOptimizer(key=optimizer_key, model=model, lr=lr, momentum=momentum)\n",
    "  start = time.time()\n",
    "  trainCNN(epoch, model, optimizer, loss_function, validationSetDataLoader, trainingSetDataLoader, \n",
    "           b=print_results_every, train_losses=train_losses, train_counter=train_counter, model_name=model_name)\n",
    "  # if (epoch % step_size == 0):\n",
    "  #   step_lr_scheduler.step()\n",
    "  end = time.time()\n",
    "  print(f\"\\nRun time per epoch: {end - start} (s) = {(end - start)/60} (mins)\\n\")\n",
    "  last_epoch_num = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8Bz4vtN7IlY"
   },
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4Gnif7s7Oc3"
   },
   "outputs": [],
   "source": [
    "# saving our models fianlly at the very end after training \n",
    "torch.save({\n",
    "            'epoch': last_epoch_num,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_losses,\n",
    "            'train_counter': train_counter\n",
    "            }, str(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXppWGz8PdI6"
   },
   "source": [
    "# Final CNN Training & Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd3R80z_RGSJ"
   },
   "outputs": [],
   "source": [
    "# Define final training loss and counter lists for graphing\n",
    "final_train_loss = []\n",
    "final_train_counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKIkUsp0PtSa"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def trainfinalCNN(epoch, model, optimizer, loss_function, train_loader, b, train_losses, train_counter):\n",
    "  for batch_id, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "    # our model is now in training phase \n",
    "    # ensure's our model will use batch norm layers and dropout layers for training\n",
    "    model.train()\n",
    "\n",
    "    # Initializing grad to 0 to ensure there is no mixing of graidents among batches\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # move a batch of images and it's labels into GPU\n",
    "    batch_data = batch_data.to(DEVICE)\n",
    "    batch_labels = batch_labels.to(DEVICE)\n",
    "    # print(torch.unique(batch_labels))\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(batch_data)\n",
    "    \n",
    "    # Calculate loss\n",
    "    # print(torch.unique(batch_labels))\n",
    "    current_loss = loss_function(outputs, batch_labels)\n",
    "\n",
    "    # Propagate error backwards\n",
    "    current_loss.backward()\n",
    "\n",
    "    # Optimize our model parameters and update scheduler\n",
    "    optimizer.step()\n",
    "\n",
    "    # Printing out our training and validation progress every so often\n",
    "    if (batch_id % b == 0):\n",
    "\n",
    "      # finding training set accuracy \n",
    "      train_correct = 0\n",
    "      train_total = 0\n",
    "      _,pred = torch.max(outputs.data, 1)\n",
    "      train_correct += (pred == batch_labels).sum()\n",
    "      train_total += batch_data.size(0)\n",
    "      current_train_acc = ((train_correct/train_total)*100)\n",
    "\n",
    "      # printing our results \n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, batch_id * len(batch_data), len(train_loader.dataset),\n",
    "          100. * batch_id / len(train_loader), current_loss.item()))\n",
    "      print(f\"Train Epoch: {epoch} Training_Accuracy: {current_train_acc}%\\n\")\n",
    "\n",
    "      train_losses.append(current_loss.item())\n",
    "      train_counter.append((batch_id*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(model.state_dict(), './model.pth')\n",
    "      torch.save(optimizer.state_dict(), './optimizer.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEJYpbz_Rcyc"
   },
   "outputs": [],
   "source": [
    "#TODO: need to finish the predict function\n",
    "# to predict on test set \n",
    "def predictOnTestSet(testSetLoader, model):\n",
    "  predictedLabels = []\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for test_batch in testSetLoader:\n",
    "      test_batch = test_batch.to(DEVICE)\n",
    "      outputs = model(test_batch)\n",
    "      # -> assign label for each sample x \n",
    "      # note: output shape: (B, Classes) and during prediction we need to add 5 back\n",
    "      \n",
    "   \n",
    "  return \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mShhdcXsnBBl"
   ],
   "machine_shape": "hm",
   "name": "CNN_Fashion_MNIST_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
